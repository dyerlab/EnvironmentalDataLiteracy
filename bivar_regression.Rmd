# Regression

<div class="chapter_image"><img src="./media/RRC3_Dock.jpg"></div>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", fig.width = 6, fig.height=5, warning=FALSE, message=FALSE)
library( ggplot2 )
theme_set( theme_bw(base_size = 14) )
```

> In this activity, we will be exploring the use of correlation and how we can get inferences about the degree of correlation between sets of data.  The most important thing to remember is that correlation does not imply causation, it is only designed to determine the way variables systematically change.

## The Data

For this example, we will go back to our old friend, the Beer Styles data set.

```{r}
data <- read.csv("./Data/Beer_Styles.csv")
summary(data)
```

Intuitively, we have been displaying data in the manner appropriate for correlations for some time using the scatter plot.  Here is an example display using the maximum Original Gravity (a measure of how much sugar is in the pre-fermented beerâ€”technically called wort) and the final level of alcohol (what the yeast makes by munching on the sugar).  These are obviously related variables, you cannot have a high alcohol fermented liquid by starting with a low sugar wort.

```{r}
library( ggplot2 )
x <- data$OG_Max
y <- data$ABV_Max
df <- data.frame( x, y )
ggplot( df, aes(x,y) ) + geom_point() + xlab("ABV") + ylab("OG")
```

The extent to which these variables change together is quantified by a correlation statistic.

```{r}
cor(x,y)
```

This correlation statistic, $\rho$, is bound between -1 (perfect negative correlation) and +1 (perfect positive correlation).  In our example here, the correlation is $\rho=$ `r format(cor(x,y),digits=3)`, suggesting that there is a positive association and it is quite strong (close to +1).  

Using a combination of either `plot()` or `geom_point()` and `cor()`, we can both display and evaluate the correlation between two variables.  For more than two, we can either iteratively go through all possible pairs and plot/evaluate them individually, or we can use the `GGally` package to plot all pairs of variables.  You may need to install the `GGally` package (it is not installed by default).  If you get an error message when you use `library(GGally)` install the package from CRAN using the command `install.packages("GGally")`.  Here I plot the maximum values for each property of the styles (and change the base font size so that it shows up properly on the PDF output).

```{r fig.height=6, warning=FALSE, message=FALSE}
library(GGally)
ggpairs(data[c(4,6,8,10,12)]) + theme_bw(base_size = 8)
```

There are three components to this graphical output.  Above the diagonal is the pair-wise correlation, on the diagonal is the density (histogram) of each variable, and below the diagonal is the plot.  The `ggpairs()` function is quite extensible.  You can specify the kinds of plots to be used above, on, and below the diagonal.  You can also mix and match different types of data and it will plot them accordingly.  Here is an example if I include the `data$Yeast` column, which is a factor.

```{r fig.height=5, warning=FALSE, message=FALSE}
ggpairs(data[c(2,4,6,8)]) + theme_bw(base_size = 8)
```

Because it is a factor, it presents the pair-wise correlations in a slightly different way.  A word of caution.  You should be very careful when using this plot with a lot of data types.  It takes a bit of time to create each of these graphical outputs and display them.  If you make the mistake of doing too many, you might as well go get some coffee because it will take a bit of time for it to finish.

## Correlations


### Parametric

The most common kind of correlation is Pearson's product moment statistic.  It is defined as:

\[
\rho = \frac{\sum_{i=1}^N(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^N(x_i-\bar{x})^2} \sqrt{\sum_{i=1}^N(x_i-\bar{x})^2}}
\]

where the numerator is a measure of the covariance between $x$ and $y$, divided by the product of the individual variable variance estimates.  The resulting parameter $r$, which approximates the statistic $\rho$, relies upon assumptions of normality.

To date, we've been using the `cor()` function to estimate correlations.  Check out the documentation on it by running.  You will see there are several options.

```{r eval=FALSE}
?cor
```

For our example data, we can estimate the correlation as:

```{r}
cor(x,y)
```

whose default is Pearson's estimator.  Simply having a correlation is not sufficient though.  Is this value significantly different than zero?  How do we know?  To evaluate our confidence in this statistic being significantly different than zero (both negative and positive), we can use the `cor.test()` function.

```{r}
cor.test(x,y)
```

Here, we have the option of setting the null hypothesis in terms of what we are testing.  This comes down to evaluating what the alternative hypothesis should be.  

1. `alternative="two.sided"`  Are we evaluating both greater than or less than?
2. `alternative="less"`  Are we only concerned about rejecting the null hypothesis if it is less
3. `alternative="greater"`  Are we only concerned with rejections if it is greater?

These alternative dictate where we determine the 'area under the curve' for assigning probabilities.

Just like in other examples, the analysis itself in `R` returns on object that is a type of variable.

```{r}
rho <- cor.test(x,y)
class(rho)
```

This type can be considered a list and you can gain access to the internal components in it.

```{r}
names(rho)
```

using the normal methods

```{r}
rho$estimate
rho$alternative
```

This can come in handy when you want to add components of an analysis to some graphical output.  In this example, I add the estimate and the probability of the correlation being significantly different than zero to a scatter plot.

```{r }
plot(x,y,xlab="The X Value", ylab="The Y Value", bty="n")
r <- paste( "r =", format( rho$estimate, digits=4) )
p <- paste( "P =", format( rho$p.value, digits=4) )
msg <- paste( r, p, sep="\n")
text( 1.050, 12, msg)
```


### Non-Parametric

In addition to parametric approaches, we have a host of non-parametric approaches that we can use to evaluate the correlation between variables.  The most common one is Spearman's Rank Correlation.  The idea here is that if your data are not conforming to assumptions of normality, you can summarize your data by ranking them instead and derive the correlation based upon the ranks of your data instead of on the raw data itself.

Here is an example where I rank both $x$ and $y$ in the `data.frame`.

```{r}
df <- df[ order(df$x),]
df$X_Rank <- 1:nrow(df)
df <- df[ order(df$y),]
df$Y_Rank <- 1:nrow(df)
df[1:10,]
```

You can see that the smallest values in $x$ are not the smallest values in $y$ but they are pretty close. in fact, the estimator for this statistic is identical to that for Pearson's $\rho$ except that instead of using the raw data, we use the rank.

\[
\rho_{Spearman} = \frac{\sum_{i=1}^N(Rx_i-\bar{Rx})(Ry_i-\bar{Ry})}{\sqrt{\sum_{i=1}^N(Rx_i-\bar{Rx})^2} \sqrt{\sum_{i=1}^N(Rx_i-\bar{Rx})^2}}
\]

```{r}
cor(x,y,method = "spearman")
```

If there are no ties in the values (e.g., how we assign a rank to a set of $x$ or $y$ values that are identical), then

\[
\rho_{Spearman} = \frac{6\sum_{i=1}^N d_i^2}{N(N^2-1)}
\]

where $d_i = Rx_i - Ry_i$ (e.g., the difference in the rank values).  If there are ties in the raw data (as we have in ours) then fractional ranks are given to all the values that have the same observation.  Look at the output above, we see that for the values of $y$ we have two that both have an assigned value of 3.6, the third and fourth observation.  To assigned tied ranks we would assign both of them a rank of 3.5.  We would assign a rank of 6 for those whose values are 3.8, etc.  If there are ties, we are warned about this when we test significance

```{r}
cor.test( x, y, method="spearman")
```

You could get around it a bit if you either increase the specificity of your measurements or throw away data.  In our case, I can do neither so it requires me to take a hit in the way in which the probability is estimated. No ties allows some simplifying assumptions to be me, whereas having them does prevents us from using them.

### Differences

So what is the difference?  Why use one over approach over the other?

In general, if they give similar responses to the same data

```{r}
cor(x,y)
cor(x,y,method="spearman")
```

Here the difference between them is `r abs(cor(x,y)-cor(x,y,method="spearman"))` with the parametric one giving a slightly larger correlation.  There is some loss of power going from observations to ranked observations.

This is not always the case though.  Consider the data^[I am making this data up, it uses a random number generator and your data is *probably* not going to produce the same identical correlation but it will be close.] below.

```{r}
a <- seq(-1.6, 1.6, by=0.05)
b <- sin(a)*20 + rnorm(length(a),sd=1)
plot(a,b)
```

As we should expect, these data are probably not normally distributed.

```{r}
qqnorm(b)
qqline(b,col="red")
```

with deviations on the tails.

```{r}
shapiro.test(b)
```

But if we look at the correlations

```{r}
cor(a,b)
cor(a,b,method="spearman")
```

in this case, the rank correlation is higher (`r abs(cor(a,b)-cor(a,b,method="spearman"))`, not a lot but enough to prove the point that parametric approximations are not <u>always</u> producing higher estimates.


### Permutation

Before we finish, I want to make a diversion into permutation.  A lot of what we do in Biology may be done on data whose underlying distributional assumptions (normality, etc.) are generally unknown.  Many times, we can make assumptions (or transform our data) in such a way as to approximate the underlying assumptions.  However, that is not always the case. In the last decade, we've relied upon permutation as a method for evaluating probabilities associated with correlations (and a whole host of other statistics) and have opened a large door onto a lot of new analyses. 

The main idea behind permutation is that you have a null hypothesis, say:

\[
H_O: \rho=0
\]

That is, you are expecting that the correlation between $x$ and $y$ is non-existent.  If this is `TRUE` then the value of $\rho$ you estimate should be just as big if you took one of your data sets, say $y$, and permuted it.  If the NULL hypothesis is `TRUE` any permutation of $y$ should produce values of $\hat{\rho}$ that are as large as you got in the original analysis.  

So, one way to test the amount of support we have in $H_O$ is to do just that.  Say we want to evaluate the significance associated with rejecting the null hypothesis of no correlation when we observed $\rho =$ `r format( cor(x,y), digits=4)`.  We can create a large number (say 999 permuted values) and look at the distribution of $\hat{\rho}$ estimates.

```{r}
# Make place to store permuted and observed values
r_null <- rep(NA,999)
# Assign observed value
r_null[1000] <- cor( x, y )
# Make 1000 permutation, each time assigning new rho
for( i in 1:999) {
  yp <- sample( y, size = length(x), replace = FALSE)
  r_null[i] <- cor( x, yp )
}
```

If we look at these values, we see that our observed correlation is way out on the right end of the NULL distribution.

```{r}
df <- data.frame( rho=r_null )
observation <- c( rep( "Permuted",999), "Observed" )
df$Observation <- factor( observation )
ggplot( df, aes(x=rho,fill=Observation)) + geom_histogram(bins=50)
```

We can evaluate the Probability of the NULL being correct as the fraction of all those values which are as large or larger than the observed one.

```{r}
P <- sum( r_null >= cor(x,y) ) / 1000
P
```

Which in this case is 1/1000!  The observed value is the largest.  You will see more and more statistical approaches that use this 'trick' (it is really a trick and drives many statisticians a bit crazy) in Biology and Ecology because the underlying distributions are largely unknown.  It does become tricky though, when you have more complicated models and there is currently a lot of research being conducted on models that have nesting (e.g., populations within regions, etc.) or other designs more complicated than the most simple ones.







## Least Squares Linear Regression

For this exercise, we will play with some different data.  In this case, we will use some data that is in the `MASS` package describing 1993 vehicles.

```{r}
library(MASS)
names(Cars93)
```

These data were taken from Car and Driver in 1993 and contain information on 93 different models that were produced that year.   


If both your predictor and response data are `numeric` and you are attempting to create a mathematical representation of one variable in terms of the other, a regression approach is appropriate.  The simplest model, containing one predictor and one response variable can be written as:

\[
y_{ij} = \beta_0 + \beta_1x_i + \epsilon_j
\]

Where $y_{ij}$ is the observed value, $\beta_0$ is the intercept term (where the line crosses the y-axis), $\beta_1$ is the slope coefficient on the $x_1$ variable, and $\epsilon_j$ is the error term--what is not explained by the model.

Least Squares Linear Regression is perhaps the most commonly used method to estimate a regression model and it is the one that is implemented in the functions below.  To estimate the model, it follows the basic steps (illustrated in the figure below).  

1. The regression model will fit a line consisting of points we will call $\hat{y}$.
2. This line will go through the mean of both the x- and y- data points ($\bar{x}$ and $\bar{y}$ respectively).
3. The slope of the line, anchored at $(\bar{x}, \bar{y})$, will be determined by finding a regression coefficient, $\beta_1$, that minimizes the sum of the distances between each observed value and its corresponding fitted value (e.g., select a $\beta_1$ that makes the smallest $\sum (y_i - \hat{y}_i)^2$).
4. Estimate $\beta_0$ the equation above using $\beta_1$ while inserting $\bar{y}$ and $\bar{x}$ for the response and predictor variables.

![Least squares regression fitting.](LeastSquaresRegression.png)

It is a rather simple approach and one that is expandable across a broad range of conditions.  However, there are some assumptions that need to be met when using a least squares linear regression.  These include:  

1. *A linear relationship.*  The terms we used above are specifically linear (e.g., there are no higher order exponents on any of the values).  If you suspect that there is a non-linear relationship between the variables, you can explore models that use modifications of the original data (e.g., $\sqrt{y}$, $x^3$, etc.).
2. *Normality of the data*.  As we saw earlier in the semester, we need to make certain assumptions about the underlying form of the data we are working with.  In this case, we will be assuming that the data we are working with conform to normal expectations.  You should test that *a priori* and we will see a bit later how it can be examined in the variation in the response not described by the model (e.g., the $\epsilon$ in the equation above).
3. *Homoscedasticity*.  Scedasticity measures the variation in a variable throughout its range.  It is important for us to look to make sure that the variation in our response variable is roughly uniform across its range (homoscedastic) as opposed to having a lot of variation on one end and little on the other (a condition of heteroscedasticity).
4. *No autocorrelation*.  For ecological data, this is an important component.  If we are collecting data, the spatial proximity of where we collect the data should not contribute to similarity in the response variable.  
5. *No colinearity among predictors*.  If you are fitting a model with more than one predictor variable, they should not be co-linear.  What does this mean?  It means that you should not use predictor variables that are highly correlated.  How high?  Well, one thought is that if $1-R^2 < 0.2$ you may need to be concerned (where $R^2$ is the fraction of the variation you can explain in one predictor by another using a linear regression approximation).


### Fitting a Linear Model

To fit a model in R, we use the function `lm()` (literally short of 'linear model') and specify the formula we are trying to fit.  In the following example, we will be looking to see the extent to which we can predict horsepower as a function of engine size.

```{r, fig.cap="Horsepower as a function of engine size (in Liters) for 93 different vehicles available in 1993.  The Mazda RX-7 is labeled as it is a rotary engine, an entirely different kind of engine than the rest."}
library(ggplot2)
p <- ggplot( Cars93, aes(x=EngineSize, y=Horsepower) ) + geom_point() 
p + geom_text( aes(label=Make, y=Horsepower+10), data=Cars93[57,])
```


As such, we specify the formula

`Horsepower ~ EngineSize`

which means, literally, that Horsepower is a function of EngineSize.

```{r}
fit <- lm( Horsepower ~ EngineSize, data=Cars93)
fit
```

The interesting terms here are the intercept ($\beta_0$) and the coefficient (the $\beta_1$) for the model fitting *Horsepower* to *EngineSize*.

As we saw previously in the case of a `cor.test()` examples, the thing that is returned from a `lm()` function call is a specific type of R object (it is really just a list), 

```{r}
class(fit)
```

one with specific names and terms contained within it.

```{r}
names(fit)
```

As before, we can use this to make a plot of the model and the data.

```{r}
plot( Horsepower ~ EngineSize, data=Cars93, xlab ="Engine Size (Liters)", ylab="Horsepower" )
abline(a=fit$coefficients[1], b=fit$coefficients[2], col="red")
b0 <- format( fit$coefficients[1], digits=3)
b1 <- format( fit$coefficients[2], digits=3)
msg <- paste( "y = ",b0, " + ", b1, "x", sep="")
text( 4.5,100, msg)
```

How does the model look?  Do you think it is a good fit?  Lets look at some output of the `lm` object.

```{r}
summary(fit)
```

We see the terms, an estimate of their magnitude, and associated probability of keeping them in the model (assuming they are not zero).  In the middle of the output, we see the R-squared values.  

\[
R^2 = \frac{SS_{Model}}{SS_{Total}}
\]

which tells us what fraction of the variation in the response variable is actually explained by the model.  A higher value for $R^2$ means that we are explaining more of the overall variation, whereas a smaller value means less of the variance is being explained.  There is a caveat here, the magnitude of $R^2$ will increase as we add more predictor variables.  To get around this, we can look at an adjusted-$R^2$, one that is corrected by the number of terms we have in the model.  The formula for that is:

\[
R^2_{Adj} = R^2 - (1-R^2)\frac{p}{N-p-1}
\]

where $R^2$ is as above, $p$ is the number of terms in the model, and $N$ is the number of observations.

In the lower part of the output, we see the estimated $F$ statistic, the degrees of freedom, and the associated P-Value.  The $F$ statistic is the test statistic for the model and is defined as the ratio of the variation explained by the model to that of the underlying data.  The more variation explained, the larger the $F$ statistic and the less likely that the model should be rejected. 

For a look at the classic ANOVA table, we see the degrees of freedom, the Sums of Squares, the Mean Squares and the estimate $F$ statistic.

```{r}
anova(fit)
```

The degrees of freedom are assigned as follows:

1. The total degrees of freedom is $N-1$.
2. You allocate a degree of freedom for each predictor variable.
3. The residual degrees of freedom (e.g. that which we did not explain) is the the rest (e.g., $N - p - 1$).

The terms for the sums of squares come from the underlying data in the following way.  The total sums of squared deviations are defined as:  

\[
SS_{Total} = \sum_{i=1}^N (y_i - \bar{y})^2
\]

which are composed of the model sums of squares (the variation in the response explained by the model)

\[
SS_{Model} = \sum_{i=1}^N (\hat{y}_i - \bar{y})^2
\]

and the residual (sometimes called error) variation 

\[
SS_{Residual} = \sum{i=1}^N (y - \hat{y}_i)^2
\]

These values denote the fit of the underlying model to the data, however, they are influenced by the number of samples that were collected.  To estimate the mean deviation in the sums of squares, a set of parameters called the Mean Squares, you divide each of the sums of squares by its degrees of freedom.

\begin{align*}
MS_{Model} &= \frac{SS_{Model}}{df_{Model}} \\
MS_{Residual} &= \frac{SS_{Residual}}{df_{Residual}} \\
MS_{Total} &= \frac{SS_{Total}}{df_{Total}} 
\end{align*}

The $MS_{Residual}$ term is our best estimator of the underlying variation in the model, whereas the $MS_{Model}$ is the variation associated with fitting the model.  The ratio of these two terms create the test statistic, $F$.

\[
F = \frac{MS_{Model}}{MS_{Residual}}
\]

This test-statistic, a ratio of variances, has a well known distribution IF the underlying data are normal.  As this is well characterized, we can look up the probability of observing a value of this statistic as large or larger than that observed.  To do this, we must know the degrees of freedom---more $df$ means larger $F$ and we need to take this into account.

The expected value of $F$ at $x$ is defined as:

\[
F_{x,df_1,df_2} = \frac{\sqrt{\frac{(df_1x)^{df_1}df_2^{df_2}}{(df_1x+df_2)^{df_1+df_2}}}}{x\mathbf{B}\left(\frac{df_1}{2},\frac{df2}{2}\right)}
\]

where $df_1$ and $df_2$ are the model and error degrees of freedom and $\mathbf{B}$ is the beta function.  

So the model 'looks' like it is one that explains a lot of the variation, ($R^2 =$ `r format(summary(fit)$adj.r.squared,digits=4)`), the amount of which appears to be of a magnitude that suggests the null hypothesis, $H_O: \beta_1 = 0$ is not true (e.g., $P < 2.2e-16$).  But is it a model that fits the assumptions and is well behaved?  We can absolutely have significant models that whose underlying data are not consistent with the assumptions.

Fortunately, the `lm()` object has some built-in plotting options that help us to diagnose the appropriateness of the model.

If you type `plot(fit)` you will be led through a series of plots depicted in the next figure.  


```{r}
par(mfrow=c(2,2))
plot(fit, which=1)
plot(fit, which=2)
plot(fit, which=3)
plot(fit, which=5)
```

These depict the following items.

1. *Residuals vs. Fitted* - This gives an indication of what is not explained by the model. Here if there are general trends (e.g., the residuals show systematic patterns of increase, decrease or non-linearity) these depict variance that is not explained by the underlying model.   
2. *Residuals QQPlot* - The variation that is not explained should also be normal, if it deviates from normality this suggests that the assumptions of the error terms may not be met in the underlying model.
3. *Scale-Location* - A measure that removes any skew in the expected values.
4. *Leverage* - A measure of the amount of differential influence that individual points may have in the estimation of the 

You must always check these values for validity in the underlying model.

## Multiple Regression

What if we have several potential predictor variables that we may want to put into a model?  How can we determine which should be added to the model and which should not?

In this example, I'm going to take vehicle weight and see if it is predicted by 

```{r}
fit.1 <- lm( Weight ~ Length, data=Cars93)
fit.2 <- lm( Weight ~ Width, data=Cars93)
fit.3 <- lm( Weight ~ EngineSize, data=Cars93)
fit.4 <- lm( Weight ~ Fuel.tank.capacity, data=Cars93)
fit.5 <- lm( Weight ~ Length + Width, data=Cars93)
fit.6 <- lm( Weight ~ Width + Fuel.tank.capacity, data=Cars93)
fit.7 <- lm( Weight ~ Length + EngineSize, data=Cars93)
fit.8 <- lm( Weight ~ Length + Fuel.tank.capacity, data=Cars93)
```

So how do we determine which of these model is better?  Look at the output from each, which do you think?


One way we could evaluate them is to look at the a
```{r}
R2 <- c( summary(fit.1)$r.squared, 
         summary(fit.2)$r.squared,
         summary(fit.3)$r.squared,
         summary(fit.4)$r.squared,
         summary(fit.5)$r.squared,
         summary(fit.6)$r.squared,
         summary(fit.7)$r.squared,
         summary(fit.8)$r.squared)
names(R2) <- paste("fit",1:8, sep=".")
R2
```


Another way to evaluate different models is through the use of a measure such as AIC (Akaike's Information Criteria) or relatives. The notion of these parameters is that there needs to be a price paid for adding additional terms to a model.  It is possible for one to add additional predictor variables and eventually inflate the observed $R^2$ for the model.  You can even add random variables and see the same thing, they will incrementally account for small amounts of variation in the response variable.  AIC is defined as:

\[
AIC = 2k - 2\log(L)
\]

where $k$ is the number of terms in the model and $L$ is an estimate of the log Likelihood estimator of the model.

```{r}
aic.vals <- c( AIC(fit.1), AIC(fit.2), 
               AIC(fit.3), AIC(fit.4), 
               AIC(fit.5), AIC(fit.6), 
               AIC(fit.7), AIC(fit.8) )
names(aic.vals) <- names(R2)
aic.vals
```

The better models are those with the smallest AIC values.  They may be positive or negative but the smallest ones are considered to be models that are better fit.  How much of a difference is considered smaller?  Well, the general approach is to estimate $\delta_{AIC}$ as the difference in magnitude of the alternative AIC values from the smallest one

```{r}
dAIC <- (aic.vals - min( aic.vals ))
dAIC
```

General consensus is that $\delta_{AIC}$ values that are between 0-2 are small enough that the models are indistinguishable---they should all be considered as equally informative.  Values of $\delta_{AIC}$ between 3-5 suggest that the models are pretty close and you may want to explore them further.  Values greater than 5 suggest that those models are not as good as the one with the smallest AIC.  

Here is a more readable output from these model tests.

```{r}
Models <- as.character( c( formula(fit.1$terms),
             formula(fit.2$terms),
             formula(fit.3$terms),
             formula(fit.4$terms),
             formula(fit.5$terms),
             formula(fit.6$terms),
             formula(fit.7$terms),
             formula(fit.8$terms)))
df <- data.frame( Model=names(dAIC),
                  Terms=Models,
                  R2=R2,
                  AIC=aic.vals,
                  Delta.AIC=dAIC)
knitr::kable(df, row.names = FALSE,digits = 3)
```


It appears that both models `fit.6` and `fit.8`, the two models with the size of the fuel tank, do a pretty reasonable job of describing the weight of the cars.  Strictly speaking `fit.6` is probably the most explanatory of the models with `fit.8` being pretty close.  























## Logistic Regression

Logistic regression is an extension of the normal regression model with the distinction that the response variable is a binary option. 

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.cap="Example of logistic regression where the predictor variable is continuous (say something like the number of homework assignments in stats completed) and the response variable is binary (such as passing stats).  The red line is an example of a regression line"}
x <- c(0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,3.25	,3.50,4.00,4.25,4.50,4.75,5.00,5.50) 
y <- c( rep(0,6), 1,0,1,0,1,0,1,0,1,1,1,1,1,1)
df <- data.frame( x, y)
ggplot( df, aes(x,y) ) + geom_point() + stat_smooth(method="glm", method.args = list(family="binomial"), se=FALSE, color="red") + xlab("Predictor Variable (Homeworks Finished)") + ylab("Binary Response Variable (Pass Class)")
fit <- glm(y~x, family=binomial(logit))
```

The underlying model is one where some probability in the response variable results in a success ($P(y=1) = \hat{p}$) or failure ($P(y=0) = 1 - \hat{p}$) is determined based upon the set of one or more predictor variables.  The regression equation in these models is thus,

\[
ln\left( \frac{\hat{p}}{1-\hat{p}}\right) = \beta_0 + \beta_1x + \epsilon
\]

Just as in the case of linear regression, we are specifying a model that has an intercept term, $\beta_0$, and a regression coefficient ($\beta_1$) on a single predictor variable ($x$). 

The expected values of a logistic model can be estimated from the coefficients as:

\[
\hat{p} = \frac{exp(\beta_0 + \beta_1x)}{1 + exp(\beta_0 + \beta_1x)}
\]

Given these expectations, we need to change the way in which we talk about the regression coefficients.  In linear regression, we interpreted a term like $\beta_1$ to indicate the *change in the response variable per unit change in the predictor*, which is not longer appropriate since we are now predicting $\hat{p}$ by a ratio of the regression models using the exponent function.  In logistic regression, we refer to the regression coefficient as the *odds ratio*.

Odds ratios are interesting and very interpretable.  The odds ratio can be estimated as $exp(\beta_1)$.  If the odds ratio of your regression coefficient equals 0.5, this indicates that per unit change in the predictor ($x$) variable the response is half as likely, an odds ratio of exactly 1.0 indicates no relationship, and one equal to 2.0 means it is twice as likely.  In the example shown in the plot above, where we are predicting passing stats by the number of homework assignments a student actually completes, the model can be specified in R as:

```{r}
fit <- glm(y~x, family=binomial(logit))
```

and the coefficients are 

```{r}
beta <- coefficients(fit)
beta
```

which translates into an odds ratio of 

```{r}
exp(beta[2])
```

meaning that it is roughly `r format(exp(beta[2]), digits=1)` times more likely to pass stats per homework that is turned in!

To examine the fit of the overall model, we can us the `anova()` function to create the classic ANOVA table testing the underlying hypothesis, $H_O: \beta=0$.

```{r}
anova(fit, test="Chisq")
```

So these data suggest that doing the homework does actually have a significant impact on the likelihood of passing stats! 

### Model Performance

Unfortunately, an exact value of $R^2$ is a bit difficult to determine using these models.  There are a few pseudo-$R^2$ approaches that have been suggested, here is on called the McFadden $R^2$.  It is in a packages that you probably do not already have (so install it using `install.packages('pscl')` if necessary).

```{r message=FALSE, warning=FALSE}
library(pscl)
pR2( fit )
```

This $R^2$ is defined as the ratio of the likelihood of the fit model over the model with only an intercept term.  In this example, the McFadden $R^2 =$`r format(as.numeric(pR2( fit )[4]),digits=2)`.

Another way to evaluate the model is to evaluate the *receiving operating characteristic* (otherwise known as ROC) which tests the classification performance of a model.  This is a technique that you will see in ecological niche modeling, particularly for MaxEnt-style models.  It is an approach that can also be visualized as the testing of the function in terms of the times it can correctly predict the outcome to the times it incorrectly predicts the outcome. For these kinds of approaches, we need to partition our data into two groups, those that are used to define the model and those that are used to test the efficacy of the model.  In our example above, we do not have enough data to do this directly.  However, in your own data (particularly if you are doing niche modeling), you must have enough data to be able to both build the model and evaluate it.  In R, the libraries `pROC` and `ROCR` allow you to do these approaches on your logistic models and I refer you to their documentation when you start playing with your data.

### An Evolutionary Example

For this one, we will step away from the Beer and Iris data sets and look at some landscape genomic data from my own laboratory.  

The data for this analysis are part of a larger data set that have been collected to look at the genetic structure and demographic/evolutionary history of a species of bark beetle in the Sonoran desert.  This particular species, *Araptus attenuatus* is only known to inhabit the senescing stems of the endemic plant, *Euphorbia lomelii*, one of the only non-pokey plants in the Sonoran desert.  

```{r echo=FALSE, eval=FALSE}
library(gstudio)
data(arapat)
data(aflp_arapat)

data <- aflp_arapat[, c("Population","loc.306.7")]
names(data)[2] <- "Locus306"


resp <- table( data$Population, data$Locus306) 
df <- data.frame( Population=factor(rownames(resp)),
                  NoAllele = resp[,1],
                  HasAllele = resp[,2] )
coords <- strata_coordinates(arapat)
names(coords)[1] <- "Population"
df <- merge( df, coords)
locus306 <- df
save(locus306, file="locus306.rda")
rm(list=ls())
```

I've saved a subset of the data as a `data.frame` that you can download from the website.  They can be loaded in and examined as:

```{r}
load("./Data/locus306.rda")
summary( locus306 )
```

Here every row is the summary of a population. For each population a set of individuals were collected and genotyped for hundreds of loci. The kind of genetic marker used here is called an AFLP (*A*mplified *F*ragment *L*ength *P*olymorphism) and the genotype given to an individual is binary, either they have a copy of the fragment or they do not.  Summarized across the population, the data record the number of individuals with a band and those without.

These samples were collected from throughout the species range and can be visualized using the normal mapping methods we covered already in class by defining a center location and grabbing a map tile from Google.

```{r message=FALSE, warning=FALSE}
library(ggplot2)
library(ggrepel)
library(ggmap)
location <- c( mean(locus306$Longitude), mean(locus306$Latitude))
map <- get_map( location, zoom=7 )
p <- ggmap( map ) + geom_point( aes(Longitude,Latitude), data=locus306 ) 
p <- p + geom_label_repel( aes(Longitude,Latitude, label=Population), data=locus306) 
p + xlab("Longitude") + ylab("Latitude")
```


### The Hypothesis

The main question here is to determine if the presence of specific alleles are associated with any particular features of the local environment.  We are invoking a question about local adaptation here and I should preface it a bit with a disclaimer.  From an evolutionary perspective, we will not be testing if the particular allele is adaptive, only if it is genetically linked to something that shows systematic changes across the landscape that are predicted by external variables.  To show 'adaptive' consequences we would have to go much further into identifying what is functionally connecting the physiology of the organism to the environmental variation.  

What we do below is acquire a set of predictor data that may or may not be associated with changes in allele presence in the desert beetle.  We will then go fit logistic models where each has a single environmental variable used to describe the landscape level patterns of allelic presence.  We can then evaluate if any of the environmental variables are associated with variation at this locus.

### Getting the Environmental Data

So, to do this, we will use some data from http://www.worldclim.org/.  These data are estimated temperature and precipitation data that has been mapped onto rasters covering the planet.  They have a resolution of ~1km at the most fine-scale.  For our example, we are sampling across hundreds of kilometers of the landscape so these kinds of data may be appropriate.  The raw temperature and precipitation data are translated into 19 well characterized bioclimatic features, hypothesized to be meaningful for large-scale ecological analyses.  The features used are: 

 - BIO1 = Annual Mean Temperature
 - BIO2 = Mean Diurnal Range (Mean of monthly (max temp - min temp))
 - BIO3 = Isothermality (BIO2/BIO7) (* 100)
 - BIO4 = Temperature Seasonality (standard deviation *100)
 - BIO5 = Max Temperature of Warmest Month
 - BIO6 = Min Temperature of Coldest Month
 - BIO7 = Temperature Annual Range (BIO5-BIO6)
 - BIO8 = Mean Temperature of Wettest Quarter
 - BIO9 = Mean Temperature of Driest Quarter
 - BIO10 = Mean Temperature of Warmest Quarter
 - BIO11 = Mean Temperature of Coldest Quarter
 - BIO12 = Annual Precipitation
 - BIO13 = Precipitation of Wettest Month
 - BIO14 = Precipitation of Driest Month
 - BIO15 = Precipitation Seasonality (Coefficient of Variation)
 - BIO16 = Precipitation of Wettest Quarter
 - BIO17 = Precipitation of Driest Quarter
 - BIO18 = Precipitation of Warmest Quarter
 - BIO19 = Precipitation of Coldest Quarter

I have downloaded and saved the 19 layers as R raster objects.  They are in a zip file on the webpage.  Download the zip and unpack it in the same file as you are currently working.

```{r eval=FALSE, echo=FALSE, message=FALSE, warning=FALSE}
library(raster)
files <- list.files("bio_22")
e <- extent( -115, -109.5, 24, 30 )
for(file in files){
  r <- raster(paste("bio_22",file, sep="/"))
  bio <- crop(r,e)
  ofile <- paste( substr(file,1,nchar(file)-7),"rda",sep=".")
  save(bio,file=ofile)
}
```

What we will do is to iterate through the files, open each raster up, and then extract the values from the raster for each of the locations we have sampled beetles from.  

This is going to use a `for-loop` structure to iterate through each file.  As such, I will make a couple caveats.   

1. We first define a vector of files to iterate through.  I've saved each of the layers are an R data object, whose file name indicates which feature it is.
2. I then make a set of `SpatialPoints` objects so that we can extract the bioclimatic data from each raster with this set.
3. The looping here is defined by the code `for( file in files)` code.  What this means is that for the data in the variable `files`, each element will be taken in turn and assigned to a variable called `file`.  The first time through, it will equal the first file name, the next time it will equal the next one, etc. until there are no more file names we haven't used in the `files` vector.
4. During each iteration, the stuff in the curly brackets following the `for` loop code will be executed.  In this case, I'm going to use the coordinates of the population in the data to `extract()` values from each raster then append these data onto our `locus306` data frame.

Finally, I should add that each of the data files that I saved are named `bio`.  So when we `load(filename)`, we are loading the raster into memory and that raster variable is named `bio`.  Look at the first one and plot it to verify.

```{r message=FALSE, warning=FALSE}
library(raster)
load("./Data/bio1.rda")
plot(bio)
```

OK, now for the looping and data acclimation.

```{r message=FALSE, warning=FALSE}
library(sp)
files <- list.files(path="./Data", pattern="*.tif", full.names = TRUE)
pts <- SpatialPoints( locus306[,c("Longitude","Latitude")] )

for( file in files ) {
  # load the raster into memory
  bio <- raster(file)
  # extract the data from each raster
  vals <- extract(bio,pts)
  # format the name of the variable from the file name
  bio_name <- substr( file, 8, nchar(file)-4)
  # insert the column of data into the data frame
  locus306[[bio_name]] <- vals
}


summary( locus306 )
```

We have 1 site where there are `NA` values. Lets clean this up by disregarding that site (this is a case where the pixel size defined boundaries on the ground such that one of our sampling locations is put into the water).

```{r}
locus306 <- locus306[ !is.na(locus306$bio1 ),]
```


Perfect!  Now we have some data to work with.  

### Fitting the Models

So now we have both the response variable (the presence/absence of particular alleles) and a set of predictor variables (the bioclimatic conditions at each population).  Our alternative hypothesis is that there is some kind of relationship between these variables such that one can predict the likelihood of an allele being present by the values of the environmental variables.  

Lets test this using the `glm()` approach we used above.  In this case, what I am going to do is to create a new `data.frame` that has the names of the bio layers as rows and for each of these we will record specifics of the models (the $\beta_1$ coefficient, pseudo $R^2$, $P$, and $AIC$).

```{r}
library(pscl)
genetic <- cbind( locus306$NoAllele, locus306$HasAllele)
df <- data.frame( Bio=names(locus306)[6:16] )
df$Beta1 <- NA
df$R2 <- NA
df$P <- NA
df$AIC <- NA

for( feature in df$Bio ){
  x <- locus306[[feature]]
  fit <- glm( genetic ~ x, family=binomial(logit))
  df$Beta1[ df$Bio == feature ] <- coefficients(fit)[2]
  df$R2[ df$Bio == feature] <- pR2( fit )[4]
  anova_table <- anova( fit, test="Chisq")
  df$P[ df$Bio == feature] <- anova_table$`Pr(>Chi)`[2]
  df$AIC[ df$Bio == feature ] <- fit$aic
}
```

Once we have these data, we can evaluate the alternative models using our pseudo $R^2$ and $AIC$ values.  I'm going to add another column to it first, the $\delta AIC$ (e.g., $AIC - min(AIC)$) to help in our comparisons and then before, printing out the table, sort it by this value.

```{r}
df$deltaAIC <- df$AIC - min( df$AIC )
df <- df[ order(df$deltaAIC),]
knitr::kable(df, row.names = FALSE)
```

From these data, we can see several things:

1. There are many models that seem to be significant (e.g., small $P$).  
2. The amount of variation explained in the allelic occurrence ranges from 0.1% to 23.9%.  
3. From a model fitting perspective, $AIC$ suggests that there are two models that we should consider (e.g., if we follow the $\delta AIC < 2.0$ suggestions), one of which is `bio18` (Precipitation of Warmest Quarter) and the other is `bio15` (Precipitation Seasonality). 
4. The best fit model estimates an *odds ratio* that per unit change in mm of precipitation during the driest quarter, the presence of the allele at this locus is `r exp(df$Beta1[1])` time more likely.

From these analyses, we can make a few statements about the underlying evolutionary question.  The features indicated both point to precipitation during the summer being something that may be physiologically linked to the location in the genome where this marker is located.  These analyses suggest that there may be some mechanism for drought tolerance or water-use efficiency or some other physiological process that is influenced by the amount (or variation) in hot quarter precipitation.  These results suggest several different subsequent analyses that may need to be performed including collecting of more data to validate the model, some laboratory experiments on the organisms to look at performance under various precipitation regimes, and further molecular genetic work to tease apart what it is actually that is causing this systematic change.   




