---
title: "Equality of Several Means"
---

<div class="chapter_image"><img src="./media/RRC2_Boat_House_From_Water.jpg"></div>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", fig.width = 6, fig.height=5, warning=FALSE, message=FALSE)
library( ggplot2 )
theme_set( theme_bw(base_size = 14) )
```


If we have more than two samples, we could do a bunch of paired $t$-test statistics but this is not the best idea.  In fact, if we do this to our data, each time testing at a confidence level of, say,  $\alpha = 0.05$, then for each time we test at $0.05$ but over all pairs, we test at an overall level of $0.05^k$ (where $k$ is the number of tests) value.  We cannot do multiple tests without penalizing ourselves in terms of the level at which we consider something significant if we are going to do all these tests.  You may have heard about a Bonferroni correction---this does exactly that, it allows us to modify the $\alpha$ level we use to take into consideration the number of tests we are going to use.  While this may be an acceptable way to test for the equality of several means (and it may not actually be if you ask most statisticians), there is another way that is much easier.  

Consider the case where we have many categories (e.g., factors in R) that we are interested in determining if the mean of all are equal.  The null hypothesis for this is, $H_O: \mu_1 = \mu_2 = \ldots = \mu_k$, where there are $k$ different treatment levels.  This is essentially what we'd want to do by doing a bunch of $t$-tests but we can use another approach that we don't have to penalize ourselves for multiple tests.  Here is how it works.

In the *Iris* data, we can visualize the means and variation around them by using box plots.  Here is an example.

```{r uni_anova}
ggplot( iris, aes(x=Species, y=Sepal.Length)) + 
  geom_boxplot(notch = TRUE) + 
  ylab("Sepal Length")
```

For us to tell if there are statistical differences among the species, we need to look at both the location of the mean values as well as the variation around them.  We do this by partitioning the variation in all the data into the components within each treatment (species) and among each treatment (species) using an approach derived from the sum of squared deviations (or Sums of Squares).  Formally, we can estimate the sum of squares within each of the $K$ groupings as:

\[
SS_{Within} = \sum_{i=1}^K\left( \sum_{j=1}^{N_i}(x_{ij}-\bar{x}_i)^2 \right)
\]

whose degrees of freedom are defined as:

\[
df_{W} = \sum_{i=1}^K \left( N_i - 1 \right) = N-K
\]

These parameters represent the deviation among samples within groups and the number of independent samples within these groups. We also need to partition out the variation among groups as a similarly defined Sums of Squares:

\[
SS_{Among} = \sum_{i=1}^K N_i\left( \bar{x}_i - \bar{x} \right)^2
\]

or the deviation among the mean of each treatment compared to the overall mean of all the data.  This parameter has degrees of freedom equal to

\[
df_{A} = K - 1
\]

These two parameters describe all the data and as such $SS_{Total} = SS_{Within} + SS_{Among}$.  Formally, we see that 

\[
SS_{Total} = \sum_{i=1}^K\sum_{j=1}^{N_i} (x_{ij} - \bar{x})^2
\]

whose degrees of freedom are 

\[
df_{T} = N - 1
\]

For each of these Sums of Squared deviations, we can standardize them using the degrees of freedom.  The notion here is that with more samples, and more treatments, we will have greater $SS$ values.  However, if we standardize these parameters by the $df$, we can come up with a standardized Mean Squared values (simplified as $MS = \frac{SS}{df}$ for each level).  

If we look at all these values, we can create the venerable ANOVA table with Among, Within, and Total partitions of the variation.

Source  | df    | SS                                                                 | MS
--------|-------|--------------------------------------------------------------------|------------------------------------------------
Among   | $K-1$ | $\sum_{i=1}^K N_i \left( \bar{x}_i - \bar{x} \right)^2$            | $\frac{SS_A}{K-1}$
Within  | $N-K$ | $\sum_{i=1}^Kn_i\left( \sum_{j=1}^{N_i}(x_{ij}-\bar{x}_i)^2 \right)$ | $\frac{SS_W}{N-K}$
Total   | $N-1$ | $\sum_{i=1}^K \sum_{j=1}^{N_i} (x_{ij} - \bar{x})^2$               | 

In R, we can evaluate the equality of means by partitioning our data as depicted above.  Essentially, if at least one of our treatments means deviate significantly, then the $MS_A$ will be abnormally large relative to the variation within each treatment $MS_W$.  This gives us a statistic, defined by the American statistician Snedekor as:

\[
F = \frac{MS_A}{MS_W}
\]

as an homage to Ronald Fisher (the F-statistic) has a pretty well understood distribution under a few conditions.  This statistic has an expectation of:

\[
f(x | df_A, df_W) = \frac{\sqrt{\frac{(df_Ax)^{df_A}df_W^{df_W}}{(df_Ax + df_W)^{df_W+df_A}}}}{x\mathbf{B}\left( \frac{df_A}{2}, \frac{df_W}{2} \right)}
\]

which is even more of a mess than that for the $t$-test!  Luckily, we have a bit of code to do this for us.

Here is an example using the *Iris* data.  Here we test the hypothesis that the Sepal Lengths are all the same (e.g., $H_O: \mu_{se} = \mu_{ve} = \mu_{vi}$)

```{r}
fit.aov <- aov( Sepal.Length ~ Species, data=iris)
fit.aov
```

The function called here, `aov()` is the one that does the Analysis of Variance.  It returns an object that has the necessary data we need.  To estimate the ANOVA table as outlined above we ask for it as:

\newpage

```{r}
anova(fit.aov)
```

which shows that the "Species" treatment are significantly different from each other, with an $F$ statistic equal to $F = 119.3$, which with 2 and 147 degrees of freedom is assigned a probability equal to $2e^{-16}$, a very small value!  

### Post-Hoc Tests

What this analysis tells us is that *at least* one of the treatment means are different from the rest.  What it does not tell us is which one or which subset.  It could be that *I. setosa* is significantly smaller than both *I. versitosa* and *I. virginia*.  It could be that *I. virginia* is significantly larger than the others, who are not different.  It could also mean that they are all different.  To address this, we can estimate a *post hoc* test, to evaluate the difference between treatment means within this model itself.  

One of the most common ways to evaluate the equality of treatment mean values is that defined by Tukey.  The so-called "Honest Significant Differences" *post hoc* test is given by

```{r}
tuk <- TukeyHSD(fit.aov)
tuk
```

which breaks down the pair-wise differences in the mean of each treatment.  Here we see the magnitude of the differences in mean values, the lower and upper confidence on the differences, and the probability associated with these differences.  In this example, all three comparisons are highly unlikely (e.g., $P$ is very small and in this case essentially zero).  As a result, we can interpret these results as suggesting that each of the three species have significantly different.  If we plot these results, we see which ones are larger and which are smaller. 

```{r}
plot( tuk )
```

Which shows the difference between treatment mean between all pairs of treatments.  Overall, we see that the *Iris* species are all significantly different.
