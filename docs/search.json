[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Environmental Data Literacy",
    "section": "",
    "text": "Preface\nThis is an open source textbook for the graudate-level course that serves as an introduction to data literacy for Environmental Studies majors at Virginia Commonwealth University.\nThis textbook serves as the main interface for this class and includes not only background materials but also slide content and online video components that support this flipped classroom experience.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Environmental Data Literacy",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI wish to thank the following individuals, who have been instrumental in the development of this document:\n\nMy thesis advisor, Dr.¬†Victoria Sork, who instilled in me the desire to dig deeper in my analytical endeavours.\nThe Rice Rivers Center for permission to use the images of the Center found on the mastehead of each chapter.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Course Learning Outcomes (CLOs)\nIn this class, we will work on all of these components using the open-source R language to explore the data analysis workflow.\nAnalysis is not a linear process, it reticulates, and has the following components.\nLearning objectives may be applied at several heirarchical levels within this course. Overall, the course has specific objectives that are a de facto statement of what you should expect to get from the content of this class. If you look to your individual degree Program Learning Outcomes, you should see how these course-level objectives map directly onto those outcomes.\nLet‚Äôs start with a definition. In this text, all definitions will be styled as follows:\nThis course has the following Course Learning Objectives (CLOs):",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#course-learning-outcomes-clos",
    "href": "Introduction.html#course-learning-outcomes-clos",
    "title": "Introduction",
    "section": "",
    "text": "TipLearning Objective\n\n\n\nLearning Objectives are explicit statements that define the knowledge, skills, and abilities you will demonstrate upon successful completion of this course. All assessments are designed to directly measure your achievement of these objectives, ensuring alignment between what you practice, what you‚Äôre evaluated on, and what you ultimately master.\n\n\n\n\nCLO-1: Use R to perform reproducible data analysis workflows across environmental contexts\n\nStudents will demonstrate functional fluency in using R and its associated libraries (e.g., Tidyverse, Quarto) for data import, transformation, visualization, and analysis, establishing a generalizable skillset for quantitative inquiry.\n\n\nBloom‚Äôs Level: Apply / Analyze\nReinforces: Seeing R as a tool for thinking and doing, not just syntax or statistical analysis\nNotes: This aligns with the practical literacy needed to ‚Äúthink with data‚Äù in a coding environment. It emphasizes generalized fluency over memorization or syntax drills.\n\n\n\nCLO-2: Analyze and interpret commonly encountered environmental data and associated analyses using appropriate exploratory and statistical techniques\n\nStudents will apply foundational exploratory and statistical approaches (e.g., binomial models, contingency tables, regression, spatial summaries) to common ecological, environmental, and evolutionary datasets to support data-driven inference.\n\n\nBloom‚Äôs Level: Analyze / Evaluate\nReinforces: Judgment in data workflows, including exploratory iteration and critique.\nNotes: This keeps the emphasis on doing the analysis and interpreting results, not on statistical derivation of model components. It fits the framing: ‚Äúnot a stats class‚Äù but ‚Äúusing common tools to make sense of real data.‚Äù It also creates space for iteration and model refinement, aligning with the ‚Äúmodel, visualize, refine‚Äù paradigm shown above.\n\n\n\nCLO-3 Communicate data-driven findings using publication-quality scientific writing and visualizations.\n\nStudents will produce clear, compelling, and reproducible documents that communicate quantitative findings, formatted according to scientific norms and using tools like Quarto and Markdown.\n\n\nBloom‚Äôs Level: Create\nReinforces: Scientific communication and agile presentation of quantitative and qualitative information in industry-standard formats.\nNotes: This grounds communication in scientific practice, where students must compose and format their insights clearly and rigorously. It ties tightly into how you assess work (‚Äúas if submitting for publication‚Äù) and emphasizes narrative data fluency, not just procedural results.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#course-modules",
    "href": "Introduction.html#course-modules",
    "title": "Introduction",
    "section": "Course Modules",
    "text": "Course Modules\nThis course is partitioned into the following four self-contained, though sequential, learning modules.\n\nModule 1: The R Ecosystem\n\nEstablishing the technical foundation for quantitative analysis in population genetics. Students will master R programming fundamentals, data visualization using the grammar of graphics, and reproducible scientific documentation. These skills are prerequisite for all subsequent evolutionary analyses.\n\n\n\nModule 2: Spatial Data\n\n‚Ä¶\n\n\n\nModule 3: Statistical Analyses",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "Introduction.html#course-level-bloom-spread-progression",
    "href": "Introduction.html#course-level-bloom-spread-progression",
    "title": "Introduction",
    "section": "Course-Level Bloom Spread Progression",
    "text": "Course-Level Bloom Spread Progression\n\n\n\n\n\n\n\n\n\n\nModule\nContent Focus\nBloom Spread\nWeight\nPattern Type\n\n\n\n\n1\nR Ecosystem (Tools)\n3.8-4.0\nX%\nSequential Mastery (within)\n\n\n2\nSpatial Data\nX-Y\nX%\nSequential Mastery (within)\n\n\n3\nStatisticsal Analyses\nX-Y\nX%\nSequential Mastery (within)\n\n\nAll\nAll Processes\n3.8 ‚Üí 5.6\n100%\nConvergent Integration",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "R_Ecosystem.html",
    "href": "R_Ecosystem.html",
    "title": "R Ecosystem",
    "section": "",
    "text": "Module Learing Objectives\nModule Bloom Spread: ~3.8-4.0\nAnalogy Summary: The process of learning these foundational skills is like learning to prepare a sophisticated meal.\nJust as a chef must master ingredients, presentation, and documentation to create reproducible haute cuisine, you must master these three objectives to produce professional, reproducible data analysis.\nThe content of this module will map onto the following sepecific module-level learning objectives (MLOs).",
    "crumbs": [
      "R Ecosystem"
    ]
  },
  {
    "objectID": "R_Ecosystem.html#module-learing-objectives",
    "href": "R_Ecosystem.html#module-learing-objectives",
    "title": "R Ecosystem",
    "section": "",
    "text": "MLO1: Foundational Data Handling & Transformation\n\nApply fundamental R data types (e.g., character, numeric, logical) and containers (e.g., vectors, lists, data frames) to load, inspect, and transform raw biological, ecological, and spatial data into structured formats using multi-step Tidyverse workflows (e.g., select, filter, mutate, summarize).\n\nContent Coverage: Understanding and applying R data types (character, numeric, logical, factor); working with data containers (vectors, lists, data frames); loading data from various formats; inspecting data structure and content; implementing Tidyverse transformation workflows; and preparing raw data for downstream analysis.\n\nMapping to CLO2 (Execute/Interpret)\n\nEstablishes the procedural skill set necessary to ‚Äúmanipulate datasets‚Äù and prepare data for analysis. Students must apply R functions and Tidyverse verbs to handle different data representations and convert raw input into structured formats required for population genetic analysis. This is the foundational ‚ÄúExecute‚Äù component‚Äîstudents cannot analyze genetic data without first being able to load, inspect, and transform it appropriately.\n\nMapping to CLO3 (Generate/Highlight)\n\nData transformation is the first step in generating scientific outputs. Students must understand that how data is structured affects what analyses are possible and how results can be communicated. Proper data handling enables subsequent generation of tables, figures, and integrated documents.\n\n\nBloom Level: ~3 (Apply - using R functions and Tidyverse workflows on new datasets)\n\n\nMLO2: Publication Quality Data Presentation\n\nImplement methods for creating publication-ready tabular and graphical data representations using knitr and ggplot2. Students will be introduced to evaluating presentation approaches (e.g., tables vs.¬†figures, chart types) to effectively communicate analytical findings, with this rhetorical decision-making reinforced throughout subsequent modules.\n\nContent Coverage: Creating tables using knitr::kable() and related functions; implementing the grammar of graphics framework using ggplot2; mapping data variables to aesthetic elements; constructing multi-layered visualizations; customizing themes and formatting; understanding when tables vs.¬†figures are most appropriate; and meeting technical specifications for publication (DPI, file formats).\n\nMapping to CLO2 (Execute/Interpret)\n\nRequires implementing established visualization and tabulation workflows using professional tools. Students execute ggplot2 code to create figures and knitr functions to format tables, interpreting which aesthetic mappings best represent their data structure.\n\nMapping to CLO3 (Generate/Highlight)\n\nEstablishes the technical foundation for CLO3 (Evidence-Based Communication) by introducing students to the concept that presentation choices are analytical decisions that shape scientific narrative. Module 1 focuses on tool mastery; later modules increasingly emphasize the justification of presentation choices. Students learn to generate visually compelling representations that highlight key patterns in data.\n\n\nBloom Level: ~3-4 (Implement/Execute with introduction to evaluative thinking about presentation choices)\n\n\nMLO3: Reproducible Analysis and Reporting\n\nProduce professional, reproducible Methods and Results documents using Quarto/Markdown by integrating narrative text, executable code chunks, numerical output, tables, and figures. Students will be introduced to publication conventions (text markup, citations, cross-references, figure legends) and code chunk options, with proficiency developing throughout subsequent modules.\n\nContent Coverage: Quarto/Markdown syntax for text formatting; integrating R code chunks with narrative text; controlling code chunk behavior (echo, eval, include); generating inline R output; formatting citations and references; creating cross-references to figures and tables; producing figure legends and captions; and rendering complete Methods and Results documents.\n\nMapping to CLO2 (Execute/Interpret)\n\nLinks technical execution (running code, troubleshooting) with required output (Methods & Results documents). Students must execute Quarto rendering workflows and interpret how code chunk options affect the final document. This establishes the connection between analysis execution and professional documentation.\n\nMapping to CLO3 (Generate/Highlight)\n\nEstablishes the technical and organizational foundation for CLO3 (Evidence-Based Communication) by linking code execution with professional documentation. Module 1 introduces the Quarto workflow and manuscript conventions; later modules increasingly emphasize the coherence and sophistication of integrated scientific arguments. Students learn to produce documents where code, results, and narrative are seamlessly integrated.\n\n\nBloom Level: ~3-4 (Produce/Implement workflows with introduction to document integration concepts)",
    "crumbs": [
      "R Ecosystem"
    ]
  },
  {
    "objectID": "R.html",
    "href": "R.html",
    "title": "1¬† Getting R",
    "section": "",
    "text": "Getting R Configured\nThe grammar of the R language was derived from another system called S-Plus. S-Plus was a proprietary analysis platform developed by AT&T and licenses were sold for its use, mostly in industry and education. Given the closed nature of the S-Plus platform, R was developed with the goal of creating an interpreter that could read grammar similar to S-Plus but be available to the larger research community. The use of R has increased dramatically due to its open nature and the ability of people to share code solutions with relatively little barriers.\nThe main repository for R is located at the CRAN Repository, which is where you can download the latest version. It is in your best interests to make sure you update the underlying R system, changes are made continually (perhaps despite the outward appearance of the website).\nThe current version of this book uses version R version 4.5.2 (2025-10-31). To get the correct version, open the page and there should be a link at the top for your particular computing platform. Download the appropriate version and install it following the instructions appropriate for your computer.\nIf you are updating your version of R from an older version, you may want to carry over the current set of libraries you have already installed to the new version. R creates a new library folder structure when you update the subversion (e.g., going from 4.3 to 4.4, the libraries are kept separate). So, before you upgrade, do the following\n# grab and save all your current packages.\npkgs &lt;- installed.packages()\npkgs &lt;- names( is.na(pkgs[,4]))\nsave(pkgs,file='~/Desktop/pkgs.rda')\nThen install the latest version of R and instruct it to look at the differences between what the default install has and what you previously had (and install the missing parts).\nnew_pkgs &lt;- installed.packages()\nnew_pkgs &lt;- names( is.na(new_pkgs[,4]))\nload('~/Desktop/pkgs.rda')\nto_install &lt;- setdiff( pkgs, new_pkgs )\ninstall.packages( to_install )\nupdate.packages()\nThis should get you up-to-date on the newest version of R.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting R</span>"
    ]
  },
  {
    "objectID": "R.html#packages",
    "href": "R.html#packages",
    "title": "1¬† Getting R",
    "section": "Packages",
    "text": "Packages\nThe base R system comes with enough functionality to get you going. By default, there is only a limited amount of functionality in R, which is a great thing. You only load and use the packages that you intend to use. There are just too many packages to have them all loaded into memory at all times and there is such a broad range of packages, it is not likely you‚Äôd need more than a small fraction of the packages during the course of all your analyses. Once you have a package, you can tell R that you intend to use it by either\n\nlibrary(package_name)\n\nor\n\nrequire(package_name)\n\nThey are approximately equivalent, differing in only that the second one actually returns a TRUE or FALSE indicating the presence of that library on you machine. If you do not want to be mocked by other users of R, I would recommend the library version‚Äîthere are situations where require will not do what you think you want it to do (even though it is a verb and should probably be the correct one to use grammatically).\nThere are, at present, a few different places you can get packages. The packages can either be downloaded from these online repositories and installed into R or you can install them from within R itself. Again, I‚Äôll prefer the latter as it is a bit more straightforward.\n\nCRAN\nThe main repository for packages is hosted by the r-project page itself. There are packages with solutions to analyses ranging from Approximate Bayesian Computation to Zhang & Pilon‚Äôs approaches to characterizing climatic trends. The list of these packages is large and ever growing. It can be found on CRAN under the packages menu. To install a package from this repository, you use the function\n\ninstall.packages(\"thePackageName\") \n\nYou can see that R went to the CRAN mirror site (I use the rstudio one), downloaded the package, look for particular dependencies that that package may have and download them as well for you. It should install these packages for you and give you an affirmative message indicating it had done so.\nAt times, there are some packages that are not available in binary form for all computer systems (the rgdal package is one that comes to mind for which we will provide a work around later) and the packages need to be compiled. This means that you need to have some additional tools and/or libraries on your machine to take the code, compile it, and link it together to make the package. In these cases, the internet and the documentation that the developer provide are key to your sanity.\n\n\nGitHub\nThere are an increasing number of projects that are being developed either in the open source community or shared among a set of developers. These projects are often hosted on http://www.github.com where you can get access to the latest code updates. The packages that I develop are hosted on Github at (http://github.com/dyerlab) and only pushed to CRAN when I make major changes.\nTo install packages from Github you need to install the remotes library from CRAN first\n\nremotes::install_github(\"USER_NAME/REPOSITORY\")",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting R</span>"
    ]
  },
  {
    "objectID": "R.html#libraries-used-in-text",
    "href": "R.html#libraries-used-in-text",
    "title": "1¬† Getting R",
    "section": "Libraries Used in Text",
    "text": "Libraries Used in Text\nThis work requires several libraries that you may need to get from either CRAN or GitHub. The following if you run the following code, you should be up-to-date on the necessary packages used throughout this text.\n\nfiles &lt;- list.files(\".\",pattern=\".qmd\")\nlibraries &lt;- c( \"gstudio\",\n                \"popgraph\",\n                \"ggplot2\", \n                \"tidyverse\", \n                \"sf\", \n                \"gt\",\n                \"terra\")\n\nfor( file in files ) {\n  suppressWarnings( s &lt;- system( paste(\"grep -E -o 'library\\\\(\\\\w+\\\\)'\",\n                                       file), \n                                  intern=TRUE))\n  if( length(s) &gt; 0 ) {\n    s &lt;- strsplit(s,\"(\",fixed=TRUE)\n    for( item in s ){\n      if( length(item) == 2){\n        library &lt;- strsplit(item[2],\")\",fixed=TRUE)[[1]]\n        libraries &lt;- c(libraries,library)\n      }\n    }\n  }\n}\n\n# Sort names\npkgs &lt;- sort( unique(libraries) )\nidx &lt;- which( pkgs == \"package_name\" )\npkgs &lt;- pkgs[-idx]\n\n# Install personal packages from GitHub\n# popgraph\nif( !require(popgraph) ) { \n  remotes::install_github(\"dyerlab/popgraph\")\n  idx &lt;- which( pkgs == \"popgraph\" ) \n  pkgs[ -idx ]\n}\n# gstudio\nif( !require(gstudio) ) { \n  remotes::install_github(\"dyerlab/gstudio\")\n  idx &lt;- which( pkgs == \"popgraph\" ) \n  pkgs[ -idx ]\n}\n\npkgs_df &lt;- data.frame(Name = pkgs, Title = NA)\nfor(i in seq_along(pkgs)){\n    f = system.file(package = pkgs[i], \"DESCRIPTION\")\n    if( nchar(f)&gt; 1) {\n        # Title is always on 3rd line\n        title = readLines(f)\n        title = title[grep(\"Title: \", title)]\n        pkgs_df$Title[i] = gsub(\"Title: \", \"\", title)    \n    }\n}\n\npkgs_df |&gt; \n  dplyr::mutate( Source = ifelse( Name %in% c( \"popgraph\",\n                                                \"gstudio\"), \n                                  \"GitHub\", \n                                  \"CRAN\" )) |&gt;\n  gt::gt()\n\n\n\nTable¬†1.1: Packages that are used in the text of this book with their official description. Those with a source of CRAN can be installed using install.packages() whereas those from GitHub must be installed using remotes::install_github().\n\n\n\n\n\n\n\n\n\nName\nTitle\nSource\n\n\n\n\nggplot2\nCreate Elegant Data Visualisations Using the Grammar of Graphics\nCRAN\n\n\ngstudio\nTools Related to the Spatial Analysis of Genetic Marker Data\nGitHub\n\n\ngt\nEasily Create Presentation-Ready Display Tables\nCRAN\n\n\npopgraph\nThis is an R package that constructs and manipulates population\nGitHub\n\n\nsf\nSimple Features for R\nCRAN\n\n\nterra\nSpatial Data Analysis\nCRAN\n\n\ntidyverse\nEasily Install and Load the 'Tidyverse'\nCRAN\n\n\n\n\n\n\n\n\n\n\nIf you are rendering this book, the snippet below will install any libraries used (from CRAN) onto your local machine.\n\ninst_pkgs &lt;- installed.packages()\nto_install &lt;- setdiff( pkgs, inst_pkgs )\nif( length(to_install)) {\n  install.packages(to_install, repos=\"https://cran.rstudio.org\")\n}",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Getting R</span>"
    ]
  },
  {
    "objectID": "Positron.html",
    "href": "Positron.html",
    "title": "2¬† Positron",
    "section": "",
    "text": "Integrated Development Environment\nWriting code in R, Python, Julia, or other languges benefit from an interface and ecosystem that helps you accomplish your data analysis tasks. By default, R comes with an interface that you could use to do all your work.\nIt is also possible to use R directly from a terminal app. If you use linux or mac, you can access R from your local terminal application. On Windows, there is a more complicated way of doing it but‚ÄîI do not undersand Windows so I cannot help with this one.\nThere are several GUI environments that you can use to interface with R beyond the built-in interface. Since February 2011, the RStudio IDE has been a popular interface for R and its development has gone hand-in-hand with the rise of the modern R reproducible reserach stack (e.g., knitr \\(\\to\\) R Markdown \\(\\to\\) tidyverse \\(\\to\\) Quarto). One of the benefits of this interface is that it made it much easier to integrate data, analysis, interpretation, and distribution into a single interface and framework‚Äîenhancing reproducibility.\nIn 2023, the next generation of IDE, named Positron focusing specifically on R, Python, Julia, and Quarto was released. This IDE is a fork of the very popular VSCode interface and is designed to support data science workflows across several languages (e.g., you can mix R, Python, Julia, SQL, etc.).\nThis text was written entirely using the Positron IDE, as a way to help me switch from RStudio in my own workflows and teaching. This book does not require you to use any specific interface‚Äîthough I will have been known to give 10,000 bonus points if you are using emacs or vi in my class‚Ä¶ üòè",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Positron</span>"
    ]
  },
  {
    "objectID": "Positron.html#integrated-development-environment",
    "href": "Positron.html#integrated-development-environment",
    "title": "2¬† Positron",
    "section": "",
    "text": "Figure¬†2.3: The main interface for RStudio.\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.4: The Positron integrated development environment showing this very page!",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Positron</span>"
    ]
  },
  {
    "objectID": "DataTypesContainers.html",
    "href": "DataTypesContainers.html",
    "title": "3¬† Data Types & Containers",
    "section": "",
    "text": "Numeric Data Types\nThe very first hurdle you need to get over is the oddness in the way in which R assigns values to variables.\nYes that is a less-than and dash character. This is the assignment operator that historically has been used and it is the one that I will stick with. In some cases you can use the ‚Äò=‚Äô to assign variables instead but then it takes away the R-ness of R itself. For decision making, the equality operator (e.g., is this equal to that) is the double equals sign ‚Äò==‚Äô. We will get into that below where we talk about logical types and later in decision making.\nIf you are unaware of what type a particular variable may be, you can always use the type() function and R will tell you.\nR also has a pretty good help system built into itself. You can get help for any function by typing a question mark in front of the function name. This is a particularly awesome features because at the end of the help file, there is often examples of its usage, which are priceless. Here is the documentation for the ‚Äòhelp‚Äô function as given by:\nThere are also package vignettes available (for most packages you download) that provide additional information on the routines, data sets, and other items included in these packages. You can get a list of vignettes currently installed on your machine by:\nand vignettes for a particular package by passing the package name as an argument to the function itself.\nThe quantitative measurements we make are often numeric, in that they can be represented as as a number with a decimal component (think weight, height, latitude, soil moisture, ear wax viscosity, etc.). The most basic type of data in R, is the numeric type and represents both integers and floating point numbers (n.b., there is a strict integer data type but it is often only needed when interfacing with other C libraries and can for what we are doing be disregarded).\nAssigning a value to a variable is easy\nx &lt;- 3\nx\n\n[1] 3\nBy default, R automatically outputs whole numbers numbers within decimal values appropriately.\ny &lt;- 22/7\ny\n\n[1] 3.142857\nIf there is a mix of whole numbers and numbers with decimals together in a container such as\nc(x,y)\n\n[1] 3.000000 3.142857\nthen both are shown with decimals. The c() part here is a function that combines several data objects together into a vector and is very useful. In fact, the use of vectors are are central to working in R and functions almost all the functions we use on individual variables can also be applied to vectors.\nA word of caution should be made about numeric data types on any computer. Consider the following example.\nx &lt;- .3 / 3\nx\n\n[1] 0.1\nwhich is exactly what we‚Äôd expect. However, the way in which computers store decimal numbers plays off our notion of significant digits pretty well. Look what happens when I print out x but carry out the number of decimal places.\nprint(x, digits=20)\n\n[1] 0.099999999999999991673\nNot quite 0.1 is it? Not that far away from it but not exact. That is a general problem, not one that R has any more claim to than any other language and/or implementation. Does this matter much, probably not in the realm of the kinds of things we do in population genetics, it is just something that you should be aware of. You can make random sets of numeric data by using using functions describing various distributions. For example, some random numbers from the normal distribution are:\nrnorm(10)\n\n [1] -2.3091187 -0.1264271 -1.1185080  0.2266867 -0.9020348 -1.0370321\n [7] -0.3766236  0.5020335 -0.5482023  2.0002796\nfrom the normal distribution with designated mean and standard deviation:\nrnorm(10,mean=42,sd=12)\n\n [1] 23.74562 40.66111 35.80823 50.16626 52.06906 66.80606 39.37287 37.97650\n [9] 31.25420 30.11576\nA poisson distribution with mean 2:\nrpois(10,lambda = 2)\n\n [1] 2 2 2 3 1 6 1 1 1 5\nand the \\(\\chi^2\\) distribution with 1 degree of freedom:\nrchisq(10, df=1)\n\n [1] 5.027570e-01 3.587836e+00 6.688119e-01 8.142454e-01 9.184860e-01\n [6] 1.048591e+01 1.210775e-01 1.263135e+00 3.347452e+00 5.633090e-05\nThere are several more distributions that if you need to access random numbers, quantiles, probability densities, and cumulative density values are available.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types & Containers</span>"
    ]
  },
  {
    "objectID": "DataTypesContainers.html#numeric-data-types",
    "href": "DataTypesContainers.html#numeric-data-types",
    "title": "3¬† Data Types & Containers",
    "section": "",
    "text": "Coercion to Numeric\nAll data types have the potential ability to take another variable and coerce it into their type. Some combinations make sense, and some do not. For example, if you load in a CSV data file using read_csv(), and at some point a stray non-numeric character was inserted into one of the cells on your spreadsheet, R will interpret the entire column as a character type rather than as a numeric type. This can be a very frustrating thing, spreadsheets should generally be considered evil as they do all kinds of stuff behind the scenes and make your life less awesome.\nHere is an example of coercion of some data that is initially defined as a set of characters\n\nx &lt;- c(\"42\",\"99\")\nx\n\n[1] \"42\" \"99\"\n\n\nand is coerced into a numeric type using the as.numeric() function.\n\ny &lt;- as.numeric( x )\ny\n\n[1] 42 99\n\n\nIt is a built-in feature of the data types in R that they all have (or should have if someone is producing a new data type and is being courteous to their users) an as.X() function. This is where the data type decides if the values asked to be coerced are reasonable or if you need to be reminded that what you are asking is not possible. Here is an example where I try to coerce a non-numeric variable into a number.\n\nx &lt;- \"The night is dark and full of terrors...\"\nas.numeric( x )\n\n[1] NA\n\n\nBy default, the result should be NA (missing data/non-applicable) if you ask for things that are not possible.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types & Containers</span>"
    ]
  },
  {
    "objectID": "DataTypesContainers.html#character-data",
    "href": "DataTypesContainers.html#character-data",
    "title": "3¬† Data Types & Containers",
    "section": "Character Data",
    "text": "Character Data\nA collection of letters, number, and or punctuation is represented as a character data type. These are enclosed in either single or double quotes and are considered a single entity. For example, my name can be represented as:\n\nprof &lt;- \"Rodney J. Dyer\"\nprof\n\n[1] \"Rodney J. Dyer\"\n\n\nIn R, character variables are considered to be a single entity, that is the entire prof variable is a single unit, not a collection of characters. This is in part due to the way in which vectors of variables are constructed in the language. For example, if you are looking at the length of the variable I assigned my name to you see\n\nlength(prof)\n\n[1] 1\n\n\nwhich shows that there is only one ‚Äòcharacter‚Äô variable. If, as is often the case, you are interested in knowing how many characters are in the variable prof, then you use the\n\nnchar(prof)\n\n[1] 14\n\n\nfunction instead. This returns the number of characters (even the non-printing ones like tabs and spaces.\n\nnchar(\" \\t \")\n\n[1] 3\n\n\nAs all other data types, you can define a vector of character values using the c() function.\n\nx &lt;- \"I am\"\ny &lt;- \"not\"\nz &lt;- 'a looser'\nterms &lt;- c(x,y,z)\nterms\n\n[1] \"I am\"     \"not\"      \"a looser\"\n\n\nAnd looking at the length() and nchar() of this you can see how these operations differ.\n\nlength(terms)\n\n[1] 3\n\nnchar(terms)\n\n[1] 4 3 8\n\n\n\nConcatenation of Characters\nAnother common use of characters is concatenating them into single sequences. Here we use the function paste() and can set the separators (or characters that are inserted between entities when we collapse vectors). Here is an example, entirely fictional and only provided for instructional purposes only.\n\npaste(terms, collapse=\" \")\n\n[1] \"I am not a looser\"\n\n\n\npaste(x,z)\n\n[1] \"I am a looser\"\n\n\n\npaste(x,z,sep=\" not \")\n\n[1] \"I am not a looser\"\n\n\n\n\nCoercion to Characters\nA character data type is often the most basal type of data you can work with. For example, consider the case where you have named sample locations. These can be kept as a character data type or as a factor (see below). There are benefits and drawbacks to each representation of the same data (see below). By default (as of the version of R I am currently using when writing this book), if you use a function like read_table() to load in an external file, columns of character data will be treated as factors. This can be good behavior if all you are doing is loading in data and running an analysis, or it can be a total pain in the backside if you are doing more manipulative analyses.\nHere is an example of coercing a numeric type into a character type using the as.character() function.\n\nx &lt;- 42\nx\n\n[1] 42\n\n\n\ny &lt;- as.character(x)\ny\n\n[1] \"42\"",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types & Containers</span>"
    ]
  },
  {
    "objectID": "DataTypesContainers.html#logical-types",
    "href": "DataTypesContainers.html#logical-types",
    "title": "3¬† Data Types & Containers",
    "section": "Logical Types",
    "text": "Logical Types\nA logical type is either TRUE or FALSE, there is no in-between. It is common to use these types in making decisions (see if-else decisions) to check a specific condition being satisfied. To define logical variables you can either use the TRUE or FALSE directly\n\ncanThrow &lt;- c(FALSE, TRUE, FALSE, FALSE, FALSE)\ncanThrow\n\n[1] FALSE  TRUE FALSE FALSE FALSE\n\n\nor can implement some logical condition\n\nstable &lt;- c( \"RGIII\" == 0, nchar(\"Marshawn\") == 8)\nstable\n\n[1] FALSE  TRUE\n\n\non the variables. Notice here how each of the items is actually evaluated as to determine the truth of each expression. In the first case, the character is not equal to zero and in the second, the number of characters (what nchar() does) is indeed equal to 8 for the character string ‚ÄúMarshawn‚Äù.\nIt is common to use logical types to serve as indices for vectors. Say for example, you have a vector of data that you want to select some subset from.\n\ndata &lt;- rnorm(20)\ndata\n\n [1] -0.07308836  0.23524767  2.76261676  0.69425933  0.56864784  0.24485685\n [7]  0.78172873 -0.34293397  0.18833885 -0.42290783 -2.02388188  0.40854168\n[13]  0.70093015  0.23559939 -0.39988112  0.25586527  0.19114274  2.44121564\n[19] -0.92991407  0.68144462\n\n\nPerhaps you are on interested in the non-negative values\n\ndata[ data &gt; 0 ]\n\n [1] 0.2352477 2.7626168 0.6942593 0.5686478 0.2448569 0.7817287 0.1883388\n [8] 0.4085417 0.7009302 0.2355994 0.2558653 0.1911427 2.4412156 0.6814446\n\n\nIf you look at the condition being passed to as the index\n\ndata &gt; 0\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE FALSE FALSE  TRUE\n[13]  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE  TRUE\n\n\nyou see that individually, each value in the data vector is being evaluated as a logical value, satisfying the condition that it is strictly greater than zero. When you pass that as indices to a vector it only shows the indices that are TRUE.\nYou can coerce a value into a logical if you understand the rules. Numeric types that equal 0 (zero) are FALSE, always. Any non-zero value is considered TRUE. Here I use the modulus operator, %%, which provides the remainder of a division.\n\n1:20 %% 2\n\n [1] 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n\n\nwhich used as indices give us\n\ndata[ (1:20 %% 2) &gt; 0 ]\n\n [1] -0.07308836  2.76261676  0.56864784  0.78172873  0.18833885 -2.02388188\n [7]  0.70093015 -0.39988112  0.19114274 -0.92991407\n\n\nYou can get as complicated in the creation of indices as you like, even using logical operators such as OR and AND. I leave that as an example for you to play with.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types & Containers</span>"
    ]
  },
  {
    "objectID": "Markdown.html",
    "href": "Markdown.html",
    "title": "4¬† Markdown",
    "section": "",
    "text": "Pandoc Supported Conversions\nIn current data analytics and communication, there are a wide variety of platforms on which we can provide summaries and insights regarding our work. Each of these end points requires a non-insignificant amount of effort to learn these systems. Moreover, they all are cul de sacs in that all the effort you exert to learn one will not allow you to get the benefits of any other platform than the one you just learned.\nEnter Pandoc, the universal document converter. It is essentially the Rosetta Stone of file types. Some really smart programmers have put together a set of software that allows you to convert from or to (and hence between) different document types given that most documents are regularly structured. With Pandoc, it does not matter if you do or do not have Word or PowerPoint or EPub or LaTeX or whatever, as long as you can create one of the supported types, you can convert that input into a huge variety of output types.\nThis is critical for us because Code is just text. Once it is evaluated, it can replaced with:\nAs such, we can embed R code within raw text to create our analyses and documents.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "Markdown.html#pandoc-supported-conversions",
    "href": "Markdown.html#pandoc-supported-conversions",
    "title": "4¬† Markdown",
    "section": "",
    "text": "Conversion Formats\n\n\n\n\nNumerical values from one or more calculations,\nTextual content from analyses or manipulation, or\nGraphical content from plots.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "Markdown.html#installing-quarto",
    "href": "Markdown.html#installing-quarto",
    "title": "4¬† Markdown",
    "section": "Installing Quarto",
    "text": "Installing Quarto\nThe first step is to go to quarto and download the quarto engine for your particular computer. If you are using Positron as your IDE, it does come stock with a version of quarto already.\nQuarto will serve as your interface between the text, code, analyses, graphics, tables, citations, and other items in your data analysis workflow. When we start on a analysis that has some kind of graphical output (tabular or charts or maps), more often than not, you‚Äôll be creating a Quarto Document. In fact, this entire book is written as a sequence of Quarto Documents (as is the website it is being hosted from).\nQuarto is your friend.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "Markdown.html#markdown-document",
    "href": "Markdown.html#markdown-document",
    "title": "4¬† Markdown",
    "section": "Markdown Document",
    "text": "Markdown Document\nA quarto document is simply a text file. Since it is text, it is easily shared and future proofed against vendor lock-in and other problems. It is also a great candidate for using version control (like GitHub and similar repositories for collaboration).\nA document has two parts: the meta data (known as the YAML), and the body.\n\nThe YAML is the part at the top and contains metadata relative to the whole document as well as instructions to what is going to happen when it gets turned into another document. By default, it has a title and a format saying that when this is rendered (by hitting that preview button above it), it will make a HTML document that is titled ‚ÄúUntitled‚Äù. For most purposes, html output is sufficient and in some cases (e.g., when deploying dynamical graphical output like maps or dashboards) preferred. You can create docx, pdf, ppt, dashboards, blogs, websites, and a host of other output from this document. Here are some rules about the YAML\n\nIt must be at the top of the document.\nIt must be enclosed by three dashes (and only three dashes) on their own line.\nItems are added as key: value and if it is textual content, enclose it in quotes.\n\nHere is a YAML from a reserach manuscript I‚Äôm working on so you can see the extra stuff that can be added.\n‚Äê‚Äê‚Äê\ntitle: |\n¬†Directional Gene Flow: Uncovering Directionality\n¬†in Population Graph Topologies\nformat: html\nauthor:\n¬†- name: Rodney J. Dyer\n¬†¬†¬†orcid: 0000-0003-4707-3453\n¬†¬†¬†corresponding: true \n¬†¬†¬†email: rjdyer@vcu.edu\n¬†¬†¬†affiliation:\n¬†¬†- name: Virginia Commonwealth University\n¬†¬†¬†department: School of Life Sciences and Sustainability\n¬†¬†¬†city: Richmond\n¬†¬†¬†state: VA\n¬†¬†¬†url: https://slss.vcu.edu\nkeywords: |\n¬†¬†¬†- Asymmetric Gene Flow\n¬†¬†¬†- Population Graphs\n¬†¬†¬†- Directionality\n¬†¬†¬†- Genetic Connectivity\ncopyright: \n¬†¬†holder: Rodney J. Dyer\n¬†¬†year: 2026\nbibliography: export.bib\nfunding: |\n¬†¬†¬†The author received no specific funding for this work.\nabstract: |\n¬†¬†¬†This manuscript develops a novel approch to estimating \n¬†¬†¬†relative directionality in genetic connetivity using a \n¬†¬†¬†Gaussian kernel approach. Stochastic individual-based \n¬†¬†¬†simulation models are used to determine the sensitivity \n¬†¬†¬†of this approach to identify asymmetry in gene flow amongst\n¬†¬†¬†a set of discrete populations showing...  The methodology\n¬†¬†¬†is applied to two case studies, one evaluating the \n¬†¬†¬†distribution of genetic variance among spatially separated \n¬†¬†¬†populations of X and another is applied to an analysis of \n¬†¬†¬†pollen pool structure. Extensions of this model to answer \n¬†¬†¬†questions in landscape and conservation genetics are \n¬†¬†¬†discussed at the end.\n‚Äê‚Äê‚Äê\n\nNotice for some things, you may need to have a list of responses and they are indented underneath the main key. See the documentation. Also, for long lines, you can use the pipe character (|) right after the key and then indent one or more lines of text underneath‚ÄîI did this here for the title, keywords, fundin, and abstract so it all displayed properly on the page without needing to scroll left/right to see it all. It is not needed in your document unless you like it to not run off the side of your editor. For specifics‚Äîfor most documents, you‚Äôll only need title, name, and format.\nAfter the YAML part, this is where your text and code goes that will be rendered and turned into the finalized document.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "Markdown.html#markdown-syntax",
    "href": "Markdown.html#markdown-syntax",
    "title": "4¬† Markdown",
    "section": "Markdown Syntax",
    "text": "Markdown Syntax\nFor maximum usability, the document that we embed our code into should be as widely available as possible‚Äîunhindered by the necessity of having a particular program just to view the content. For this, R uses Markdown, created by John Gruber & Aaron Swartz in 2004. Markdown was created so that people are enabled ‚Äú‚Ä¶to write using an easy-to-read and easy-to-write plain text format‚Ä¶‚Äù\nBecause everything is text, it is easy share and collaborate using Markdown, and for R, it is how we can make a wide array of output document types including (but not limited to):\n\nConventional documents (PDF, Word, RTF, etc.)\nHTML pages with interactive elements (this document here is an interactive html document).\nPresentations (LaTeX, PowerPoint, JavaScript, etc.)\nDashboards with interactive content.\n\nWhen we make a document, presentation, or any other output, there are only a finite set of different text components we put into the document. The document itself does not need to be heavy or bloated, it is just text (though surprisingly, a blank Word document on my laptop with nothing in the document itself is still 12KB in size!). Common elements include:\n\nHeaders & Titles\nTypography such as italic, bold, underline, strike through\nLists (numbered or as bullets)\nPictures and links\nPage numbers, tables of contents, etc.\n\nWhat Markdown does is allows you to type these components and use ‚Äòmarking‚Äô around the elements to make them different from regular text. It is really, amazingly simple.\n\nHeadings\nTitle and headers are created by prepending a hashtag\n# Header 1\n## Header 2\n### Header 3\n#### Header 4\nare converted into the following headers styles (in docx, html, pdf, etc.).\n\n\n\n\n\n\nFigure¬†4.1: Example heading formats based on the styles used in this document.\n\n\n\n\n\nText Styles\nThe actual appearance of the headers are determined by where it is being presented (e.g., in Word it will take the default typography and font attributes, etc.). You are more than welcome to customize these settings using style sheets in a variety of formats (see documentation).\nIndividual paragraphs are delimited by either a blank line between them or two spaces at the end of the sentence. I find it much easier to just put the\n\n\n\nMarkdown\nRendered As\n\n\n\n\nplain text\nplain text\n\n\n*italic*\nitalic\n\n\n**bold**\nbold\n\n\n~~strike through~~\nstrike through\n\n\n\n\n\n\n\n\n\nEmbedded Content\nYou can also embed links and images. Both of these are configured in two parts. For links, you need to specify the text that can be clicked upon and it must be surrounded in square brackets. The link to the web or file or image is right next to the square brackets and is contained within parentheses. So the markdown.\n[rodneydyer.com](https://rodneydyer.com)\nbecomes (when rendered)\nrodneydyer.com\nImages are the same except that the whole thing is prepended by an exclamation mark. The stuff you put into the square brackets are either the alt-text or in the case of this document, the figure legend.\nSo,\n![This is the goat.](https://www.flickr.com/photo_download.gne?id=55027807032&secret=9a6f698983&size=o)\nis rendered as:\n\n\n\nThis is the goat.\n\n\n\n\nLists\nLists (both numbered and unordered) are created using dashes or asterisks.\n\nWill be turned into an unordered list as:\n\nBullet 1\n\nBullet 2\n\nBullet 3\n\nWhereas the following raw text.\n\nWill be rendered in list format as:\n\nFirst\n\nSecond\n\nThird\n\nActually, you can just use 1. in front of every line if you like, it will auto-number them for you when it makes a list. I tend to do this because it makes it a bit easier in case I want to reorder the list later and I don‚Äôt have to go back and change the numbers.\n\n\nFootNotes\nYou can add a footnote using two components. First, you need to insert the location where you want the footnote to annotate. You do this by enclosing square brackets and a carat symbol (^) where you want to put the footnote in the text and an identifier number (e.g., [^1]). This will designate a link in the text that will allow you to jumpt to the bottom of the page.\nThe next part, the text or content you are going to display needs to be indicated as well. This can be right after the identifying item or at the very end of the text itself. I typically localize the text content near where the superscript is located so they are linked. To indicate to Quarto that this is to be rendered as a footnote, you use the same number that you used for the superscript and enclose it in square brackets and a carat, followed by a colon.\n[^1]:. This is the content of the footnote I made above.\nThis will set up the content that is put into the end of the page (and provide a link back to the location in the document that you jumped from).\nHere is an example1.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "Markdown.html#code-text",
    "href": "Markdown.html#code-text",
    "title": "4¬† Markdown",
    "section": "Code & Text",
    "text": "Code & Text\nOn of the strengths of Quarto and its integration of R and markdown is the ability to mix code and text together in one place. This allows us to bring all of our analyses and data as close to one another as possible, helping with reproducibility and error reduction. You do not have to do your analysis in one program, copy-and-paste the results to your word processor, then copy and paste to your presentation software, then go to your graphics program and make a figure and copy-and-paste it togehter, etc. It all lives in one place.\n\nCode Chunks\nQuarto supports code chunks (or code cells in Positron), which can be one or many lines of raw R code. This code is executed and the results are merged into the markdown in the document (text, graphical, interactive widgets, whatever) before knitting. You can think of this as a little bit of an R script that you can insert into your document.\nEach chunk is enclosed within boundary rows, the top row must contain three acute accents (back ticks - `) followed by the letter r in curly brackets ```{r}. This back tick is the grave (`) character and not a single quote (on the US English keyboard it is the key in the top left corner).\nThe end of the chunk is indicated by three back ticks ``` on their own line. Everything between these two enclosing lines is treated as R code and is subject to evaluation when you re-knit the document.\n\n\n\n\n\n\nWarning\n\n\n\nIf you mess up the leading and trailing lines of a chunck, the rendering process will get totally hosed in both the IDE (it does not know what to style as text versus code parts) and the rendering (same reasons).\n\n\nHere is what a chunk looks like in markdown that prints out a simple message.\n```{r}\nprint(\"This is text from a chunk\")\n```\n\nWhen it is evaluated, the R interpreter removes the first and last rows, and executes the code within them. By default, the code is presented as a box (gray background in this example) in the document as well as any output that is produced from the code itself.\n\nprint(\"This is text from a chunk\")\n\n[1] \"This is text from a chunk\"\n\n\nNotice, the output from the chunk is shown in the line [1] right after the chunk. Anything you print or show in a chunk will be displayed directly below the chunk itself.\n\nChunk Options\nThere are several options we can apply to a chunk. These can be set at the top of the document, in the YAML, which will impact all the chuncks in the document, or on a per-chunk basis. Here some options you will commonly use.\n\n\n\n\n\n\n\nOption\nDescription\n\n\n\n\necho\nShows the chunk in the final document (T/F).\n\n\neval\nExecute the code in the chunk (T/F).\n\n\nwarning\nShow any code warnings in the output (T/F).\n\n\nerror\nShow any code errors in the output (T/F).\n\n\n\nTo put these in an individual chunk, place them at the top of the chunk (this is important, they should be the first lines of text) preceded by #|. For example, the following chunk will show its contentws, not evaluate the code, and suppress any warnings or errors when it does execute.\n\nfor( i in 1:100000000000000) { \n    print(\"hello world!\")\n}\n\nIt should be noted that some libraries produce copious amounts of junk when you load them in. For these, warning: false is not sufficient and you‚Äôll have to wrap them inside a suppressPackageStartupMessages() call.\n\nsuppressPackageStartupMessages( library( tidyverse ) ) \n\nIn my workflow, I typically develop the document with all my chunks showing\n\n\n\nInline Code\nYou can easily integrate code, into the text, either to be displayed OR to be evaluated. For example, in R you get the value of \\(\\pi\\) by the constant pi. Type that into the console and it will return 3.1415927.\nIf you look at the RMarkdown for that paragraph above, it looks like the following before knitting:\nYou can easily integrate code, into the text, either to \nbe displayed *OR* to be evaluated. For example, in `R` \nyou get the value of $\\pi$ by the constant `pi`. \nType that into the console and it will return 3.1415927.\nNotice the following parts:\n\nSymbols: The \\(\\pi\\) symbol is created by the name of the symbol surrounded by dollar signs. $ $. There are a ton of symbols and equations you can use, all borrowed from LaTeX, so if you need complicated equations or symbols, this is not a problem.\nText rendered as code (in typography) but not evaluated: Both the `R` and the `pi` are examples here. Nothing is evaluated, but it looks like code.\nEvaluated R Code: Any code between r and the ending ` will be evaluated as R code within the text. When you render/preview the document the document, it will be run and the contents between these symbols and replaced the content by the output of the R code itself.. So, the markdown `r pi` will be replaced in the text as 3.1415927 as if you actually wrote out the digits of \\(pi\\).\n\nThis is HUGE! Think of this as how you will never have to update numerical values in your manuscripts again if they come from your anlayses.\n\n\n\n\n\nTip\n\n\n\nYou will never have to change numerical values in your text that are derived from statistical analyses again since you can link the variables (and associated values) in your R code to be displayed in the text as if you typed them yourself. If your analysis changes‚Ä¶ your text is changed. Magically.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "Markdown.html#tables-figures",
    "href": "Markdown.html#tables-figures",
    "title": "4¬† Markdown",
    "section": "Tables & Figures",
    "text": "Tables & Figures",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "Markdown.html#margin-content",
    "href": "Markdown.html#margin-content",
    "title": "4¬† Markdown",
    "section": "Margin Content",
    "text": "Margin Content",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "Markdown.html#footnotes",
    "href": "Markdown.html#footnotes",
    "title": "4¬† Markdown",
    "section": "",
    "text": "This is the content of the footnote I made above.‚Ü©Ô∏é",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Markdown</span>"
    ]
  },
  {
    "objectID": "FactorData.html",
    "href": "FactorData.html",
    "title": "5¬† Factor Data",
    "section": "",
    "text": "Factors\nA factor is a categorical data type. If you are coming from SAS, these are class variables. If you are not, then perhaps you can think of them as mutually exclusive classifications. For example, an sample may be assigned to one particular locale, one particular region, and one particular species. Across all the data you may have several species, regions, and locales. These are finite, and defined, sets of categories. One of the more common headaches encountered by people new to R is working with factor types and trying to add categories that are not already defined.\nSince factors are categorical, it is in your best interest to make sure you label them in as descriptive as a fashion as possible. You are not saving space or cutting down on computational time to take shortcuts and label the locale for Rancho Santa Maria as RSN or pop3d or 5. Our computers are fast and large enough, and our programmers are cleaver enough, to not have to rename our populations in numeric format to make them work (hello STRUCTURE I‚Äôm calling you out here). The only thing you have to loose by adopting a reasonable naming scheme is confusion in your output.\nTo define a factor type, you use the function factor() and pass it a vector of values.\nregion &lt;- c(\"North\",\"North\",\"South\",\"East\",\"East\",\"South\",\"West\",\"West\",\"West\")\nregion &lt;- factor( region )\nregion\n\n[1] North North South East  East  South West  West  West \nLevels: East North South West\nWhen you print out the values, it shows you all the levels present for the factor. If you have levels that are not present in your data set, when you define it, you can tell R to consider additional levels of this factor by passing the optional levels= argument as:\nregion &lt;- factor( region, levels=c(\"North\",\"South\",\"East\",\"West\",\"Central\"))\nregion\n\n[1] North North South East  East  South West  West  West \nLevels: North South East West Central\nIf you try to add a data point to a factor list that does not have the factor that you are adding, it will give you an error (or ‚Äòbarf‚Äô as I like to say).\nregion[1] &lt;- \"Bob\"\nNow, I have to admit that the Error message in its entirety, with its \"[&lt;-.factor(*tmp*, 1, value = ‚ÄúBob‚Äù)‚Äú` part is, perhaps, not the most informative. Agreed. However, the‚Äùinvalid factor level‚Äù does tell you something useful. Unfortunately, the programmers that put in the error handling system in R did not quite adhere to the spirit of the ‚Äúfail loudly‚Äù mantra. It is something you will have to get good at. Google is your friend, and if you post a questions to (http://stackoverflow.org) or the R user list without doing serious homework, put on your asbestos shorts!\nUnfortunately, the error above changed the first element of the region vector to NA (missing data). I‚Äôll turn it back before we move too much further.\nregion[1] &lt;- \"North\"\nFactors in R can be either unordered (as say locale may be since locale A is not &gt;, =, or &lt; locale B) or they may be ordered categories as in Small &lt; Medium &lt; Large &lt; X-Large. When you create the factor, you need to indicate if it is an ordered type (by default it is not). If the factors are ordered in some way, you can also create an ordination on the data. If you do not pass a levels= option to the factors() function, it will take the order in which they occur in data you pass to it. If you want to specify an order for the factors specifically, pass the optional levels= and they will be ordinated in the order given there.\nregion &lt;- factor( region, ordered=TRUE, levels = c(\"West\", \"North\", \"South\", \"East\") )\nregion\n\n[1] North North South East  East  South West  West  West \nLevels: West &lt; North &lt; South &lt; East",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Factor Data</span>"
    ]
  },
  {
    "objectID": "FactorData.html#factors",
    "href": "FactorData.html#factors",
    "title": "5¬† Factor Data",
    "section": "",
    "text": "Missing Levels in Factors\nThere are times when you have a subset of data that do not have all the potential categories.\n\nsubregion &lt;- region[ 3:9 ]\nsubregion\n\n[1] South East  East  South West  West  West \nLevels: West &lt; North &lt; South &lt; East\n\n\n\ntable( subregion )\n\nsubregion\n West North South  East \n    3     0     2     2",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Factor Data</span>"
    ]
  },
  {
    "objectID": "Tidyverse.html",
    "href": "Tidyverse.html",
    "title": "6¬† Tidyverse",
    "section": "",
    "text": "The Data\nFor this topic we will use some example data from the Rice Rivers Center. These data represent both atmospheric and water data collected from instrumentation on-site. I have stored these data in a spreadsheet that is shared on Google Drive as a CSV file.\nYou can look at it here.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "Tidyverse.html#verbs-of-analysis",
    "href": "Tidyverse.html#verbs-of-analysis",
    "title": "6¬† Tidyverse",
    "section": "Verbs of Analysis",
    "text": "Verbs of Analysis\nWhen we perform any type of data manipulation, we use specific verbs. There is a limited lexicon for us to use, but the key here is how we perform these actions, and in which order they are deployed for a huge diversity in outcomes. For now, these basic verbs include:\n\nSelect: Used to grab or reorder columns of data.\nFilter: Used to grab subsets of records (rows) based upon some criteria.\nMutate: Create new columns of data based upon manipulations of existing columns.\nArrange: Order the records (rows) based upon some criteria.\nGroup: Gather records together to perform operations on chunks of them similar to by().\nSummarize: Extract summaries of data (or grouped data) based upon some defined criteria.\n\nIn the following examples, we‚Äôll be using the rice data above. For each verb, I‚Äôm going to use the pipe operator (%&gt;%) to send the data into the example functions and then assign the result to a dummy data.frame named df. The arguments passed to each of the verbs are where the magic happens.\n\nThe Output\nThe key to these activities is that every one of these functions takes a data.frame as input, does its operations on it, then return a data.frame object as output. The data.frame is the core data container for all of these actions.\n\n\nSelect Operator\nThe select() function allows you to choose which columns of data to work with.\n\nrice %&gt;%\n  select( DateTime, AirTempF ) -&gt; df \nhead(df)\n\n# A tibble: 6 √ó 2\n  DateTime             AirTempF\n  &lt;chr&gt;                   &lt;dbl&gt;\n1 1/1/2014 12:00:00 AM     31.0\n2 1/1/2014 12:15:00 AM     30.7\n3 1/1/2014 12:30:00 AM     31.2\n4 1/1/2014 12:45:00 AM     30.5\n5 1/1/2014 1:00:00 AM      30.9\n6 1/1/2014 1:15:00 AM      30.6\n\n\nSelect can also be used to reorder the columns in a data.frame object. Here are the names of the data columns as initially loaded.\n\nnames( rice )\n\n [1] \"DateTime\"                       \"RecordID\"                      \n [3] \"PAR\"                            \"WindSpeed_mph\"                 \n [5] \"WindDir\"                        \"AirTempF\"                      \n [7] \"RelHumidity\"                    \"BP_HG\"                         \n [9] \"Rain_in\"                        \"H2O_TempC\"                     \n[11] \"SpCond_mScm\"                    \"Salinity_ppt\"                  \n[13] \"PH\"                             \"PH_mv\"                         \n[15] \"Turbidity_ntu\"                  \"Chla_ugl\"                      \n[17] \"BGAPC_CML\"                      \"BGAPC_rfu\"                     \n[19] \"ODO_sat\"                        \"ODO_mgl\"                       \n[21] \"Depth_ft\"                       \"Depth_m\"                       \n[23] \"SurfaceWaterElev_m_levelNad83m\"\n\n\nLet‚Äôs say that you wanted to reorder the columns as RecordID, ODO_mgl and PH as the first three columns and leave everything else as is. There is this cool function everthying() that helps out.\n\nrice %&gt;%\n  select( RecordID, ODO_mgl, PH, everything() ) -&gt; df\nnames( df )\n\n [1] \"RecordID\"                       \"ODO_mgl\"                       \n [3] \"PH\"                             \"DateTime\"                      \n [5] \"PAR\"                            \"WindSpeed_mph\"                 \n [7] \"WindDir\"                        \"AirTempF\"                      \n [9] \"RelHumidity\"                    \"BP_HG\"                         \n[11] \"Rain_in\"                        \"H2O_TempC\"                     \n[13] \"SpCond_mScm\"                    \"Salinity_ppt\"                  \n[15] \"PH_mv\"                          \"Turbidity_ntu\"                 \n[17] \"Chla_ugl\"                       \"BGAPC_CML\"                     \n[19] \"BGAPC_rfu\"                      \"ODO_sat\"                       \n[21] \"Depth_ft\"                       \"Depth_m\"                       \n[23] \"SurfaceWaterElev_m_levelNad83m\"\n\n\n\n\nFilter\nThe function filter() works to select records (rows) based upon some criteria. So for example, if I am interested in just records when the airtemp was freezing (and the raw data are in ¬∞F). The range of values in the original data was:\n\nrange( rice$AirTempF )\n\n[1]  3.749 74.870\n\n\nbut after filtering using the name of the variable and a logical operator.\n\nrice %&gt;%\n  filter( AirTempF &lt; 32 ) -&gt; df\nrange( df$AirTempF )\n\n[1]  3.749 31.990\n\n\nJust like select(), it is possible to have several conditions, that are compounded (using a logical AND operator) by adding them to the filter() function. Here I also split the conditionals requiring the data to be above freezing air temperatures, not missing data from the PH meter, and water turbidity &lt; 15 ntu‚Äôs. I also put each of these onto their own lines and auto-indent does a great job of making it reasonably readable.\n\nrice %&gt;%\n  filter( AirTempF &gt; 32, \n          !is.na(PH), \n          Turbidity_ntu &lt; 15) -&gt; df\nnrow(df)\n\n[1] 1449\n\n\n\n\nMutate\nThe mutate() function changes values in the table and is quite versatile. Here I will jump back to our old friend mdy_hms() from lubridate and convert the DateTime column, which is\n\nclass( rice$DateTime )\n\n[1] \"character\"\n\n\nand convert it into a real date and time object\n\nrice %&gt;%\n  mutate( Date = mdy_hms(DateTime, tz = \"EST\") ) -&gt; df\nclass( df$Date )\n\n[1] \"POSIXct\" \"POSIXt\" \n\nsummary( df$Date )\n\n                 Min.               1st Qu.                Median \n\"2014-01-01 00:00:00\" \"2014-01-22 08:22:30\" \"2014-02-12 16:45:00\" \n                 Mean               3rd Qu.                  Max. \n\"2014-02-12 16:45:00\" \"2014-03-06 01:07:30\" \"2014-03-27 09:30:00\" \n\n\nYou can also create several mutations in one mutation step.\n\nrice %&gt;%\n  mutate( Date = mdy_hms(DateTime, tz = \"EST\"), \n          Month = month(Date, label = TRUE) ) -&gt; df\nsummary( df$Month )\n\n Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec \n2976 2688 2535    0    0    0    0    0    0    0    0    0 \n\n\n\n\nArrange\nWe can sort entire data.frame objects based upon the values in one or more of the columns using the arrange() function.\n\nrice %&gt;%\n  arrange( WindSpeed_mph ) -&gt; df \ndf$WindSpeed_mph[1]\n\n[1] 0\n\n\nBy default, it is in ascending order, to reverse it, use the negative operator on the column name object in the function.\n\nrice %&gt;%\n  arrange( -WindSpeed_mph ) -&gt; df \ndf$WindSpeed_mph[1]\n\n[1] 30.65\n\n\nAs above, it is possible to combine many columns of data as criteria for sorting by adding more arguments to the function call.\n\nrice %&gt;%\n  arrange( -WindSpeed_mph, WindDir ) -&gt; df\n\n\n\nSummarise\nThis function is the first one that does not return some version of the original data that was passed to it. Rather, this performs operations on the data and makes a brand new data.frame object.\nEach argument you give to the function performs one or more operations on the data and returns a brand new data.frame object with only the the values specified.\nHere is an example where I am taking the mean air and water temperature (n.b., one is in ¬∞F and the other is in ¬∞C). Notice the result is a new data.frame object with one row and two new columns defined by how I asked for the summary in the first place. I used single tick notation so I can have a space in the column names.\n\nrice %&gt;%\n  summarize( `Air Temp` = mean( AirTempF), \n             `Water Temp` = mean(H2O_TempC, na.rm=TRUE))\n\n# A tibble: 1 √ó 2\n  `Air Temp` `Water Temp`\n       &lt;dbl&gt;        &lt;dbl&gt;\n1       38.8         5.53\n\n\n\n\nGroup & Summarize\nTo get more than one row in the resulting data.frame from summary(), we need to group the data in some way. The function group_by() does this and is used prior to summary(). Let‚Äôs take a look at how we can get the average air and water temp by month. To do this, I‚Äôm going to have to do several steps. I‚Äôm just going to chain them together using the %&gt;% operator.\n\nrice %&gt;%\n  mutate( Date = mdy_hms( DateTime, \n                          tz=\"EST\"),\n          Month = month( Date, \n                         abbr = FALSE, \n                         label=TRUE) ) %&gt;%\n  group_by( Month ) %&gt;%\n  summarize( `Air Temp` = mean( AirTempF), \n             `Water Temp` = mean( H2O_TempC, \n                                  na.rm=TRUE) )\n\n# A tibble: 3 √ó 3\n  Month    `Air Temp` `Water Temp`\n  &lt;ord&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1 January        34.7         3.68\n2 February       39.7         5.29\n3 March          42.6         7.96\n\n\nAs you read the code, notice how easy it is to understand what is going on because of both the pipes and because of the way I am formatting the code itself.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "Tidyverse.html#flows",
    "href": "Tidyverse.html#flows",
    "title": "6¬† Tidyverse",
    "section": "Flows",
    "text": "Flows\nThis last part really showed off the process of multi-step data manipulations using the pipe operator and the several verbs we introduced. These are both efficient in terms of typing as well as efficient in the way of producing research that makes sense to look at.\nHere are some strategies that I use when building up these manipulation workflows.\n\nDo not think that you have to do the whole thing at once. I typically build up the workflow, one line at a time. Make sure the output from the previous line is what you think it should be then add the next one.\nKeep your code open and airy, it makes it easier to read and to catch any logical errors that may arrise.\nYou can pipe into a lot of different functions. In fact, any function that takes a data frame can be the recipient of a pipe. While developing a workflow, I will often pipe into things like head(), summary(), or View() to take a look at what is coming out of my workflow to make sure it resembles what I think it should look like.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Tidyverse</span>"
    ]
  },
  {
    "objectID": "TabularData.html",
    "href": "TabularData.html",
    "title": "7¬† Tabular Data",
    "section": "",
    "text": "Markdown Tables\nThere are several ways to make tabular output in your files: manual and from code.\nWhen I say manual, I mean that the table is written specifically in markdown and rendered as a table. For basic tables, we use the pipe character (|) and dashes (‚Äê) to outline the table. We designate the header row by having a table row underneath it with dashes in it. The ‚Äòcolumns‚Äô do not need to be the same size, but for me, it always looks better if they are. If you would like to have a caption on the table and a reference to it, put that right under the table where the row of text starts with a colon and the reference is in curley brackets and it will put the caption on the top (proper place for tables) and add a numbered table prefix to it (n.b., a table reference must start with \\#tbl- to be recognized as a table reference).\nWhich will produce the table:\nThen we can reference the table in the text using the @- citation format, here that would be @tbl-example, and would be rendered as Table¬†7.1 in the final document.\nYou can stylize the contents of the table cells using normal markdown but there is not a lot of customization that can be applied to structure of the tables beyond specify the justification of the contents within each table. By default, it center justifies a column but we can use a colon (:) inside the row that has the dashes to make it left, center, or right justified as well. Here is how that works.\nHere is an example using the table from above.\nWhich will left-justify the first two columns, center justify the Time column, and right justify the Cost column.\nCompare Table¬†7.1 and Table¬†7.2 to see how we can customize justifications.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Tabular Data</span>"
    ]
  },
  {
    "objectID": "TabularData.html#markdown-tables",
    "href": "TabularData.html#markdown-tables",
    "title": "7¬† Tabular Data",
    "section": "",
    "text": "| Header | Description     | Time  | Cost  |\n|--------|-----------------|-------|-------|\n| First  | The stuff       |  1    | 3.32  |\n| Second | The other stuff |  3    | 12.32 |\n: This is the header for the table. {#tbl-example}\n\n\n\n\n\nTable¬†7.1: This is the header for the table.\n\n\n\n\n\nHeader\nDescription\nTime\nCost\n\n\n\n\nFirst\nThe stuff\n1\n3.32\n\n\nSecond\nThe other stuff\n3\n12.32\n\n\n\n\n\n\n\n\n\nA single colon on the left side justifies to the left,\nTwo colons, on on the left and one on the right produces center justifcation, and\n\nA single colon on the right side of the column justifies the contents to the right.\n\n\n| Header | Description     | Time  | Cost  |\n|:-------|:----------------|:-----:|------:|\n| First  | The stuff       |  1    | 3.32  |\n| Second | The other stuff |  3    | 12.32 |\n: This is tabel with left, center, and right justification. {#tbl-example2}\n\n\n\n\n\nTable¬†7.2: This is tabel with left, center, and right justification.\n\n\n\n\n\nHeader\nDescription\nTime\nCost\n\n\n\n\nFirst\nThe stuff\n1\n3.32\n\n\nSecond\nThe other stuff\n3\n12.32",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Tabular Data</span>"
    ]
  },
  {
    "objectID": "TabularData.html#rendered-tables",
    "href": "TabularData.html#rendered-tables",
    "title": "7¬† Tabular Data",
    "section": "Rendered Tables",
    "text": "Rendered Tables\nMost of the tabular output we see will be derived from data. The key here is to make the data.frame object represent the columns and rows of data that you want displayed and then hand that off to one of several libraries that can make a table for you. Here I am going to use the gt library, but feel free to check out the knitr & kableExtra libraries for a similarly awesome approach.\nHere is a data.frame as an example that takes the mean petal length and width from the built-in iris data set. I‚Äôve set up the data.frame to look exactly like I would like to show up in the output using some common dplyr actions.\n\nlibrary( tidyverse )\n\niris |&gt;\n    group_by( Species ) |&gt;\n    summarize( Length = mean( Petal.Length),\n                Width = mean( Petal.Width) ) |&gt; \n    mutate( Species = paste( \"I.\", Species )) |&gt;\n    rename( `Iris Species` = Species) -&gt; data\n\ndata \n\n# A tibble: 3 √ó 3\n  `Iris Species` Length Width\n  &lt;chr&gt;           &lt;dbl&gt; &lt;dbl&gt;\n1 I. setosa        1.46 0.246\n2 I. versicolor    4.26 1.33 \n3 I. virginica     5.55 2.03 \n\n\nTo make a table from this data frame, just pipe it to the gt() function.\n\nlibrary( gt )\ndata |&gt;\n    gt() \n\n\n\nTable¬†7.3: The mean lenght and with of Fishers classic three Iris species.\n\n\n\n\n\n\n\n\n\nIris Species\nLength\nWidth\n\n\n\n\nI. setosa\n1.462\n0.246\n\n\nI. versicolor\n4.260\n1.326\n\n\nI. virginica\n5.552\n2.026\n\n\n\n\n\n\n\n\n\n\nBy default, it shows an alternating row colors and left justifies all the numerical data with a reasonable number of digits.\nThere is a ton of customizations available and I encourage you to go look at the documentation. Here I italicize the species column and identify specific cells and rows of the table based on the values inside.\n\ndata |&gt;\n    gt() |&gt;\n    tab_style(\n        style = list(\n            cell_text(style = \"italic\")\n        ),\n        locations = cells_body(\n            columns = `Iris Species`\n        )\n    ) |&gt;\n    tab_style( \n        style = list( \n            cell_fill( color = \"lightcyan\"),\n            cell_text( weight = \"bold\")\n        ),\n        locations = cells_body(\n            column = Length,\n            rows = Length == min(Length)\n        )\n    ) |&gt;\n    tab_style( \n        style = list( \n            cell_text( color = \"red\")\n        ),\n        locations = cells_body(\n            rows = (Length*Width) == max(Length*Width)\n        )\n    )\n\n\n\nTable¬†7.4: The mean lenght and with of Fishers classic three Iris species with some customization in the text columns and highlights in the numerical data. The length entry that is the smallest is in bold with a cyan cell fill color and the species row with the largest leaf area is shown with red text.\n\n\n\n\n\n\n\n\n\nIris Species\nLength\nWidth\n\n\n\n\nI. setosa\n1.462\n0.246\n\n\nI. versicolor\n4.260\n1.326\n\n\nI. virginica\n5.552\n2.026\n\n\n\n\n\n\n\n\n\n\nThis output can be exported to html, docx, rtf, \\(\\LaTeX\\), etc.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Tabular Data</span>"
    ]
  },
  {
    "objectID": "Graphics.html",
    "href": "Graphics.html",
    "title": "8¬† Graphics",
    "section": "",
    "text": "The Data\nThe iris flower data set (also known as Fisher‚Äôs Iris data set) is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper entitled, The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.\nThese data are part of the base R distribution and contain sepal and pedal measurements for three species if congeneric plants, Iris setosa, I. versicolor, and I. virginica.\nHere is what the data summary looks like.\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Graphics</span>"
    ]
  },
  {
    "objectID": "Graphics.html#the-data",
    "href": "Graphics.html#the-data",
    "title": "8¬† Graphics",
    "section": "",
    "text": "The three species of iris in the default data set.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Graphics</span>"
    ]
  },
  {
    "objectID": "Graphics.html#classic-r-graphics",
    "href": "Graphics.html#classic-r-graphics",
    "title": "8¬† Graphics",
    "section": "Classic R Graphics",
    "text": "Classic R Graphics\nThe base R comes with several built-in plotting functions, each of which is accessed through a single function with a wide array of optional arguments that modify the overall appearance.\nHistograms - The Density of A Single Data Vector\n\nhist( iris$Sepal.Length)\n\n\n\n\n\n\n\n\nYou can see that the default values for the hist() function label the x-axis & title on the graph have the names of the variable passed to it, with a y-axis is set to ‚ÄúFrequency‚Äù.\n\nxlab & ylab: The names attached to both x- and y-axes.\nmain: The title on top of the graph.\nbreaks: This controls the way in which the original data are partitioned (e.g., the width of the bars along the x-axis).\n\nIf you pass a single number, n to this option, the data will be partitioned into n bins.\nIf you pass a sequence of values to this, it will use this sequence as the boundaries of bins.\n\ncol: The color of the bar (not the border)\nprobability: A flag as either TRUE or FALSE (the default) to have the y-axis scaled by total likelihood of each bins rather than a count of the numbrer of elements in that range.\n\n\nDensity Plots\nEstimating the continuous density of data\n\nd_sepal.length &lt;- density( iris$Sepal.Length )\nd_sepal.length\n\n\nCall:\n    density.default(x = iris$Sepal.Length)\n\nData: iris$Sepal.Length (150 obs.); Bandwidth 'bw' = 0.2736\n\n       x               y           \n Min.   :3.479   Min.   :0.000148  \n 1st Qu.:4.790   1st Qu.:0.034088  \n Median :6.100   Median :0.153218  \n Mean   :6.100   Mean   :0.190407  \n 3rd Qu.:7.410   3rd Qu.:0.378921  \n Max.   :8.721   Max.   :0.396476  \n\n\nThe density() function estimates a continuous probability density function for the data and returns an object that has both x and y values. In fact, it is a special kind of object.\n\nclass(d_sepal.length)\n\n[1] \"density\"\n\n\nBecause of this, the general plot() function knows how to plot these kinds of things.\n\nplot( d_sepal.length )\n\n\n\n\n\n\n\n\nNow, the general plot() function has A TON of options and is overloaded to be able to plot all kinds of data. In addition to xlab and ylab, we modify the following:\n\ncol: Color of the line.\nlwd: Line width\nbty: This covers the ‚Äòbox type‚Äô, which is the square box around the plot area. I typically use bty=\"n\" because I hate those square boxes around my plots (compare the following 2 plots to see the differences). But you do you.\nxlim & ylim: These dictate the range on both the x- and y-axes. It takes a pair of values such as c(min,max) and then limits (or extends) that axis to to fill that range.\n\n\n\nScatter Plots\nPlotting two continuous variables\n\nplot( iris$Sepal.Length, iris$Sepal.Width  )\n\n\n\n\n\n\n\n\nHere is the most general plot(). The form of the arguments to this function are x-data and then y-data. The visual representation of the data is determined by the optional values you pass (or if you do not pass any optional values, the default is the scatter plot shown above)\n\n\n\n\n\n\n\nParameter\nDescription\n\n\n\n\ntype\nThe kind of plot to show (‚Äôp‚Äôoint, ‚Äôl‚Äôine, ‚Äôb‚Äôoth, or ‚Äôo‚Äôver). A point plot is the default.\n\n\npch\nThe character (or symbol) being used to plot. There 26 recognized general characters to use for plotting. The default is pch=1.\n\n\ncol\nThe color of the symbols/lines that are plot.\n\n\ncex\nThe magnification size of the character being plot. The default is cex=1 and deviation from that will increase (\\(cex &gt; 1\\)) or decrease (\\(0 &lt; cex &lt; 1\\)) the scaling of the symbols.\n\n\nlwd\nThe width of any lines in the plot.\n\n\nlty\nThe type of line to be plot (solid, dashed, etc.)\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the relevant things you can use the parameter pch for is to differentiate between groups of observations (such as different species for example). Instead of giving it one value, pass it a vector of values whose length is equal to that for x- and y-axis data.\nHere is an example where I coerce the iris$Species data vector into numeric types and use that for symbols.\n\nsymbol &lt;- as.numeric(iris$Species)\nsymbol\n\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[149] 3 3\n\n\n\nplot( iris$Sepal.Length, iris$Sepal.Width, pch=symbol )\n\n\n\n\n\n\n\n\nWe can use the same technique to use col instead of pch. Here I make a vector of color names and then use the previously defined in the variable symbol.\n\nraw_colors &lt;- c(\"red\",\"gold\",\"forestgreen\")\ncolors &lt;- raw_colors[ symbol ]\ncolors[1:10]\n\n [1] \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\" \"red\"\n\n\nIn addition to the general form for the function plot(x,y) we used above, we can use an alternative designation based upon what is called the functional form. The functional form is how we designate functions in R, such as regression anlaysis. This basic syntax for this is y ~ x, that is the response variable (on the y-axis) is a function of the predictor (on the x-axis).\nFor simplicty, I‚Äôll make x and y varibles pointing to the same same data as in the previous graph.\n\ny &lt;- iris$Sepal.Width\nx &lt;- iris$Sepal.Length\n\nThen, the plot() function can be written as (including all the fancy additional stuff we just described):\n\nplot( y ~ x , \n      col=colors, \n      pch=20, \n      bty=\"n\", \n      xlab=\"Sepal Length\", ylab=\"Sepal Width\")\n\n\n\n\n\n\n\n\nThis is much easier to read (also notice how I used serveral lines to put in all the options to the plot function for legibility).\n\n\nBar Plots\nThe barplot function takes a set of heights, one for each bar. Let‚Äôs quickly grab the mean length for sepals across all three species. There are many ways to do this, here are two, the first being more pedantic and the second more concise.\nThe iris data is in a data.frame that has a column designating the species. We can see which ones using unique().\n\nunique( iris$Species )\n\n[1] setosa     versicolor virginica \nLevels: setosa versicolor virginica\n\n\nTo estimate the mean for each species, we can take values in iris$Sepal.Length for each level of iris$Species using indices.\n\nmu.Setosa &lt;- mean( iris$Sepal.Length[ iris$Species == \"setosa\" ])\nmu.Versicolor &lt;- mean( iris$Sepal.Length[ iris$Species == \"versicolor\" ])\nmu.Virginica &lt;- mean( iris$Sepal.Length[ iris$Species == \"virginica\" ])\n\nmeanSepalLength &lt;- c( mu.Setosa, mu.Versicolor, mu.Virginica )\nmeanSepalLength\n\n[1] 5.006 5.936 6.588\n\n\nWhen we plot these data using barplot() we pass the values and set the names of the bars us\n\nbarplot( meanSepalLength, \n         names.arg = c(\"setosa\",\"versicolor\",\"virginica\"), \n         xlab=\"Iris Species\",\n         ylab=\"Mean Sepal Length\")\n\n\n\n\n\n\n\n\nThe second way to do this is to use the by() function (see ?by for the complete help file). The by function takes the following objects:\n\nThe raw data to use as measurements. Here we will use iris$Sepal.Length as the raw data.\nData designating groups to partition the raw data into (we will use iris$Species).\nThe function that you want to use on each group. (here we will ask for the mean).\n\n\nmeanSepalLength &lt;- by( iris$Sepal.Length, iris$Species, mean )\nmeanSepalLength\n\niris$Species: setosa\n[1] 5.006\n------------------------------------------------------------ \niris$Species: versicolor\n[1] 5.936\n------------------------------------------------------------ \niris$Species: virginica\n[1] 6.588\n\n\nThe data returned from this function is both numeric and has a name set for each value.\n\nis.numeric( meanSepalLength )\n\n[1] TRUE\n\nnames( meanSepalLength )\n\n[1] \"setosa\"     \"versicolor\" \"virginica\" \n\n\nThen when we pass that to barplot() the column labels are set automatically (e.g., no need to set names.arg as above).\n\nbarplot( meanSepalLength, \n         xlab = \"Iris Species\",\n         ylab = \"Average Sepal Length\")\n\n\n\n\n\n\n\n\n\n\nBoxplots\nA boxplot contains a high amount of information content and is appropriate when the groupings on the x-axis are categorical. For each category, the graphical representation includes:\n\nThe median value for the raw data\nA box indicating the area between the first and third quartile (e.g,. the values enclosing the 25% - 75% of the data). The top and bottoms are often referred to as the hinges of the box.\nA notch (if requested), represents confidence around the estimate of the median.\nWhiskers extending out to shows \\(\\pm 1.5 * IQR\\) (the Inner Quartile Range)\nAny points of the data that extend beyond the whiskers are plot as points.\n\nFor legibility, we can use the functional form for the plots as well as separate out the data.frame from the columns using the optional data= argument.\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         notch=TRUE, \n         ylab=\"Sepal Length\" )\n\n\n\n\n\n\n\n\n\n\nColors\nNamed Colors -  There are 657 pre-defined, named colors built into the base R distribution. Here is a random selection of those values.\n\nrandomColors &lt;- sample( colors(), size = nrow(iris) )\nhead(randomColors)\n\n[1] \"chocolate4\"        \"slategray1\"        \"blue3\"            \n[4] \"navajowhite\"       \"grey100\"           \"mediumspringgreen\"\n\n\nTo use these colors, you can specify them by name for either all the elements\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         col = randomColors[1],\n         notch=TRUE, \n         ylab=\"Sepal Length\" )\n\n\n\n\n\n\n\n\nor for each element individually.\n\nboxplot( Sepal.Length ~ Species, \n         data = iris, \n         col = randomColors[1:3],\n         notch=TRUE, \n         ylab=\"Sepal Length\" )\n\n\n\n\n\n\n\n\nHex Colors:  You can also use hexadecimal representations of colors, which is most commonly used on the internet. A hex representation of colors consists of red, green, and blue values encoded as numbers in base 16 (e.g., the single digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F). There are a lot of great resources on the internet for color themes that report red, green, blue and hex values. I often use the coolors.co website to look for themes that go well together for slides or presentations.\nColor Brewer Finally, there is an interesting website at colorbrewer2.org that has some interesting built-in palettes. There is an associated library that makes creating palettes for plots really easy and as you get more expreienced with R, you will find this very helpful. For quick visualizations and estimation of built-in color palettes, you can look at the website (below).\n or look at the colors in R\n\nlibrary(RColorBrewer)\ndisplay.brewer.all()\n\n\n\n\n\n\n\n\nThere are three basic kinds of palettes: divergent, qualitative, and sequential. Each of these built-in palletes has a maximum number of colors available (though as you see below we can use them to interpolate larger sets) as well as indications if the palette is safe for colorblind individuals.\n\nbrewer.pal.info\n\n         maxcolors category colorblind\nBrBG            11      div       TRUE\nPiYG            11      div       TRUE\nPRGn            11      div       TRUE\nPuOr            11      div       TRUE\nRdBu            11      div       TRUE\nRdGy            11      div      FALSE\nRdYlBu          11      div       TRUE\nRdYlGn          11      div      FALSE\nSpectral        11      div      FALSE\nAccent           8     qual      FALSE\nDark2            8     qual       TRUE\nPaired          12     qual       TRUE\nPastel1          9     qual      FALSE\nPastel2          8     qual      FALSE\nSet1             9     qual      FALSE\nSet2             8     qual       TRUE\nSet3            12     qual      FALSE\nBlues            9      seq       TRUE\nBuGn             9      seq       TRUE\nBuPu             9      seq       TRUE\nGnBu             9      seq       TRUE\nGreens           9      seq       TRUE\nGreys            9      seq       TRUE\nOranges          9      seq       TRUE\nOrRd             9      seq       TRUE\nPuBu             9      seq       TRUE\nPuBuGn           9      seq       TRUE\nPuRd             9      seq       TRUE\nPurples          9      seq       TRUE\nRdPu             9      seq       TRUE\nReds             9      seq       TRUE\nYlGn             9      seq       TRUE\nYlGnBu           9      seq       TRUE\nYlOrBr           9      seq       TRUE\nYlOrRd           9      seq       TRUE\n\n\nIt is very helpful to look at the different kinds of data palettes available and I‚Äôll show you how to use them below when we color in the states based upon population size at the end of this document.\n\n\nPlot Annotations\nYou can easily add text onto a graph using the text() function. Here is the correlation between the sepal length and width (the function cor.test() does the statistical test).\n\ncor &lt;- cor.test( iris$Sepal.Length, iris$Sepal.Width )\ncor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Sepal.Width\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\n\nWe can put the correlation and the p-value on the plot\n\ncor.text &lt;- paste( \"r = \", format( cor$estimate, digits=4), \"; P = \", format( cor$p.value, digits=4 ), sep=\"\" ) \ncor.text\n\n[1] \"r = -0.1176; P = 0.1519\"\n\n\nThe we can the overlay this onto an existing plot. For the text() function, we need to give the x- and y- coordinates where you want it put onto the coordinate space of the existing graph.\n\nplot( y ~ x , \n      col=colors, \n      pch=20, \n      bty=\"n\", \n      xlab=\"Sepal Length\", ylab=\"Sepal Width\")\ntext( 7.4, 4.2, cor.text )",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Graphics</span>"
    ]
  },
  {
    "objectID": "Graphics.html#ggplot2-graphics",
    "href": "Graphics.html#ggplot2-graphics",
    "title": "8¬† Graphics",
    "section": "GGPlot2 Graphics",
    "text": "GGPlot2 Graphics\n\nThe ggplot2 library is a gramattical approach to data display.\n\nIn base R, the graphics are generally produced by adding a lot of optional arguments to a single function such as plot() or barplot() or boxplot(). We can get some kinds of overlays using text() or points() or lines() but there is not a cohesive framework for setting this up. For even moderately complex graphical display, these approaches become unwieldy when we have to cram all that information into extra optional arguments.\n\n\n\nbase plot in R\n\n\nConsider the graph below whose data are from a 2011 article in The Economist measuring human development and perception of corruption for 173 countries (Figure¬†8.1). Both the amount of data and the way in which the data are displayed (physically and aesthetically) are somewhat complex.\n\n\n\n\n\n\n\n\nFigure¬†8.1: Corruption and Human Develoment among OECD countries from The Economist magazine.\n\n\n\n\n\nThis graphic is constructed from several additive components1 including:\n\nA raw data.frame that has several kinds of data (CPI, HDI, region, names, etc.).\n\nA aesthetic statement indicating which columns of data to use and how to use them in the plot (designating x-axis vs color, etc.).\n\nAn estimate of a trendline through the data (the red one), which displays a statistical summary of the raw data.\n\nA set of geometric overlays for the points which include size and shape configurations.\n\nSpecified color scheme for the regions.\nLabeling of a subset of the data (which is done using a separate data.frame derived from the first).\nLabels on axes.\nA legend positioned in a specific fashion.\n\nA title over the whole thing.\n\nA theme for the rest of the coloring and customized lines and grids.\n\nTruth be told (and you can look at the RMD of this file to verify), this one graphic required 42 relatively terse lines of code to construct! If all of that code was stuffed into the optional arguments for a few functions, I think I would go mad.\nLuckily for us, there are people who spend a lot of time working on these issues and thinking about how to best help us effectively display data. One of these individuals was Leland Wilkinson, whose book The Grammar of Graphics defined just such a system.\n\n\n\nThe Grammar of Graphics by Leland Wilkinson\n\n\nThis philosophy has been inserted into the R Ecosystem by Hadley Wickham in the ggplot2 library, which is descbribed as:\n\nA system for ‚Äòdeclaratively‚Äô creating graphics, based on ‚ÄúThe Grammar of Graphics‚Äù. You provide the data, tell ‚Äòggplot2‚Äô how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details.\n\nThroughout the majority of this course, we will be using this library and this approach for all but the most trivial of graphical displays.\n\nBasic ggplot\nAs outlined above, the basis of this appraoch is an additive (and iterative) process of creating a graphic. This all starts with the data. For our purposes, we will use the same iris data.frame as in the previous section on base graphics.\n\n\n\nThe iris data\n\n\n\nsummary( iris )\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50  \n                \n                \n                \n\n\nWe start building a graphic using the ggplot() function and passing it the data.frame object. This will initialize the graphic, though it will not plot anything.\n\nlibrary(ggplot2)\n\nggplot( iris )\n\n\n\n\n\n\n\n\nNext, we need to tell the plot which variables it will be using from the data.frame. For simplicity, we do not need to make special data objects with just the variables we want to plot, we can pass around the whole data.frame object and just indicate to ggplot which ones we want to use by specifying the aesthetics to be used.\n\nggplot( iris , aes( x=Sepal.Length ) )\n\n\n\n\n\n\n\n\nAt this point, there is enough information to make an axis in the graph because the underlying data has been identified. What has not been specified to date is the way in which we want to represent the data. To do this, we add geometries to the graph. In this case, I‚Äôm going to add a histogram\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram()\n\n\n\n\n\n\n\n\nNow we have a base graph!\n\n\nAestheics and Scope\nThe location of the data and the aes() determines the scope of the assignment. What I mean by this is:\n\nIf the data and aes() is in the the ggplot() function, then everything in the whole plot inherits that assignment.\nIf you put them in one or more of the components you add to ggplot() then the they are localized to only those layers.\n\nSo the following statements are all identical for this most basic of plots.\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram()\nggplot( iris ) + geom_historgram( aes(x=Sepal.Length) )\nggplot() + geom_histogram( aes(x=Sepal.Length), data=iris)\n\n\nIn the first case, the geom_histogram() inherits both data and aesthetics from ggplot().\n\nIn the second one, it inherits only the data but has it‚Äôs own specification for aesthetics.\nIn the last one, ggplot() only specifies the presence of a graph and all the data and aesthetics are localized within geom_histogram() function.\n\nWhere this becomes important is when we want to make more complicated graphics like the one above. The data that has the country CDI and HDI also has the names of the countries. However, only a subset of the country names are plot. This is because both the geometric layer and the text layer that has the names are using different data.frame objects.\nHere is a more simplistic example where I overlay a density plot (as a red line) on top of the histogram.\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram() + geom_density( col=\"red\")\n\n\n\n\n\n\n\n\nBoth the geom_histogram and the geom_density use the same data and same specification for how to deal with the y-axis. However, the density is depicted as a frequency on the y-axis whereas the histogram uses counts. Also notice how the col=\"red\" is localized just for the geom_density() layer.\nWe can override the way in which geom_histogram uses the y-axis by changing the aesthetics for that particular geometric layer. Here, I‚Äôm goint to add another aes() just within the geom_histogram() function and have it treat y as the density rather than the count (yes that is two periods before and after the word density).\n\nggplot( iris, aes(x=Sepal.Length) ) + geom_histogram(aes(y=..density..)) + geom_density( col=\"red\" )\n\n\n\n\n\n\n\n\nBy default, everything inside the ggplot() function call is inherited by all the remaining components unless it is specifically overridden. Here is a more pedantic version where only the raw data.frame is in the ggplot and the rest is in each of the geometric layers.\n\nggplot( iris ) + \n  geom_histogram( aes(x=Sepal.Length, y=..density..) ) + \n  geom_density( aes(x=Sepal.Length), col=\"red\", lwd=2)\n\n\n\n\n\n\n\n\n\n\nLabels & Titles\nJust like we added geometric layers to the plot to make histograms and densities, we do the same for labels and titles.\n\nggplot( iris,  aes(x=Sepal.Length) ) + \n  geom_histogram( aes(y=..density..), bins = 10, fill=\"lightgray\", col=\"darkgrey\" ) + \n  geom_density( col=\"red\", lwd=1.5) + \n  xlab(\"Length\") + ylab(\"Density\") + \n  ggtitle(\"Sepal Lengths for Three Iris Species\")\n\n\n\n\n\n\n\n\n\n\nScatter Plots\nWith two columns of data, we can make the old scatter plot using the geom_point() function.\n\nggplot( iris, aes(x=Sepal.Length, y=Sepal.Width) ) + geom_point( col=\"purple\") \n\n\n\n\n\n\n\n\nIn this plot, we are hiding some of the information by having all the points be the same color and shape. We could have a geom_point for each species as follows:\n\nggplot(  ) + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ 1:50,], col=\"red\") + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ 51:100,], col=\"yellow\" ) + \n  geom_point( aes( x = Sepal.Length, y = Sepal.Width), data=iris[ iris$Species == \"virginica\", ], col=\"darkgreen\" ) \n\n\n\n\n\n\n\n\nBut that is a lot of typing. In cases like this, where there is a an actual column of data that we want to use to change the appearance (e.g., in this case the Species column), we can put this within the aes() directly and ggplot() will handle the specifics for you. Anything we do to reduce the amount of typing we must do is going to help us be more accurate analysts.\n\nggplot( iris, aes( x = Sepal.Length, y = Sepal.Width, col=Species) ) + geom_point()\n\n\n\n\n\n\n\n\n\n\nIn or Out of aes()\nNotice in the last graph I put the name of the data column in the aesthetic but have the color (col) within the aes() function call in the graph before that, I put color outside of the aes() in the geom_point() function. What gives? Here is a simple rule.\n\nIf information from within the data.frame is needed to customize the display of data then it must be designated within the aes(), whereas if the display of the data is to be applied to the entire geometric layer, it is specified outside of the aes() call.\n\nHere is an example, where I have the color of the shapes determined by a value in the data.frame but have the shape2 applied to all the points, independent of any data in the data.frame.\n\nggplot( iris ) + geom_point(aes( x = Sepal.Length, y = Sepal.Width, col=Species), shape=5)\n\n\n\n\n\n\n\n\nWe can build these things in an iterative fashion making things easier to read. In what follows I will use the basic plot from above but assign it to the variable p as I add things to it. It can be as iterative as you like and you can add a bunch of stuff and wait until the end to display it.\n\np &lt;- ggplot( iris ) \np &lt;- p + geom_point(aes( x = Sepal.Length, y = Sepal.Width, col=Species, shape=Species), size=3, alpha=0.75 ) \np &lt;- p + xlab(\"Sepal Length\") \np &lt;- p + ylab(\"Sepal Width\")\n\nThe overall class of the plot varible is\n\nclass(p)\n\n[1] \"ggplot2::ggplot\" \"ggplot\"          \"ggplot2::gg\"     \"S7_object\"      \n[5] \"gg\"             \n\n\nAnd there is no plot output until we display it specifically.\n\np\n\n\n\n\n\n\n\n\n\n\nThemes\nThe overall coloration of the plot is determined by the theme.\n\np + theme_bw()\n\n\n\n\n\n\n\n\n\np + theme_dark()\n\n\n\n\n\n\n\n\n\np + theme_minimal()\n\n\n\n\n\n\n\n\n\np + theme_linedraw()\n\n\n\n\n\n\n\n\n\np + theme_void()\n\n\n\n\n\n\n\n\nYou can even define your own themes to customize all the text and lines.\nOne thing that I like to do is to specify a default theme for all my plots. You can accomplish this using theme_set() and from this point forward, this theme will be used as the default (again, we need to try as hard as possible to minimzie the amount of typing we do to minimize the amount of mistakes we make).\n\ntheme_set( theme_bw() )\n\n\n\nBoxplots\n\nggplot( iris, aes( x = Sepal.Length) ) + geom_boxplot( notch=TRUE )\n\n\n\n\n\n\n\n\n\nggplot( iris, aes(x=Species, y=Sepal.Length) )  + geom_boxplot( notch=TRUE )\n\n\n\n\n\n\n\n\n\n\nCustom Color Palettes\nTODO: FILL THIS OUT\n\nlibrary( wesanderson)\n\n\n\nOverlays\nJust like in the previous\n\np &lt;- ggplot( iris, aes(Sepal.Length, Sepal.Width) ) + \n  geom_point(col=\"red\") + \n  xlab(\"Sepal Length\") + \n  ylab(\"Sepal Width\")\n\nThe order by which you add the components to the ggplot() will determine the order of the layers from bottom to top‚Äîthe. Layers added earlier will be covered by content in layers that are added later. Compare the following plot that takes the length and width of the sepals and overlays a linear regression line over the top.\n\np + geom_point(col=\"red\") + \n  stat_smooth( formula = y ~ x, method=\"lm\", alpha=1.0)\n\n\n\n\n\n\n\n\nCompare that plot to the one below. Notice how puting stat_smooth() in front of the call to geom_point() layes the regression smoothing line and error zone underneath the points.\n\np + stat_smooth(formula = y ~ x, method=\"lm\", alpha=1.0) + \n  geom_point(col=\"red\") \n\n\n\n\n\n\n\n\n\n\nLabeling\nWe can create two kinds of annotations, text on the raw graph and text associated with some of the points. Labels of the first kind can be added direclty by placing raw data inside the aes() function.\nI‚Äôll start by taking the correlation between sepal width and length.\n\ncor &lt;- cor.test( iris$Sepal.Length, iris$Sepal.Width )\ncor\n\n\n    Pearson's product-moment correlation\n\ndata:  iris$Sepal.Length and iris$Sepal.Width\nt = -1.4403, df = 148, p-value = 0.1519\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.27269325  0.04351158\nsample estimates:\n       cor \n-0.1175698 \n\n\nAnd then grab the raw data from it and make a message.\n\ncor.text &lt;- paste( \"r = \", format( cor$estimate, digits=4), \"; P = \", format( cor$p.value, digits=4 ), sep=\"\" ) \ncor.text\n\n[1] \"r = -0.1176; P = 0.1519\"\n\n\nThat I‚Äôll stick onto the graph directly\n\np + geom_text( aes(x=7.25, y=4.25, label=cor.text))\n\n\n\n\n\n\n\n\nAlternatively, we may want to label specific points. Here I find the mean values for each species.\n\nmean_Length &lt;- by( iris$Sepal.Length, iris$Species, mean, simplify = TRUE)\nmean_Width &lt;- by( iris$Sepal.Width, iris$Species, mean, simplify = TRUE)\nmean_Values &lt;- data.frame(  Species = levels( iris$Species), \n                            Sepal.Length = as.numeric( mean_Length ), \n                            Sepal.Width = as.numeric( mean_Width ) ) \nmean_Values\n\n     Species Sepal.Length Sepal.Width\n1     setosa        5.006       3.428\n2 versicolor        5.936       2.770\n3  virginica        6.588       2.974\n\n\nTo plot and label these mean values, I‚Äôm going to use two steps. First, since I named the columns of the new data.frame the same as before, we can just inherit the aes() but substitute in this new data.frame and add label=Species to the the aesthetics.\n\np + geom_text( data=mean_Values, aes(label=Species) )\n\n\n\n\n\n\n\n\nBut that is a bit messy. Here is a slick helper library for that that will try to minimize the overlap.\n\nlibrary( ggrepel ) \np + geom_label_repel( data=mean_Values, aes(label=Species) )\n\n\n\n\n\n\n\n\nSlick.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Graphics</span>"
    ]
  },
  {
    "objectID": "Graphics.html#footnotes",
    "href": "Graphics.html#footnotes",
    "title": "8¬† Graphics",
    "section": "",
    "text": "Literally, we add these toghter using the plus ‚Äò+‚Äô sign just like we were going to develop an equation.‚Ü©Ô∏é\nThe shapes are the same as the pch offerings covered in the lecture on graphing using Base R routines here.‚Ü©Ô∏é",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Graphics</span>"
    ]
  },
  {
    "objectID": "Functions.html",
    "href": "Functions.html",
    "title": "9¬† Functions",
    "section": "",
    "text": "A Basic Function\nOne of the most fundamentally valuable things with R is that it is totally extensible by the user community. This is why there are literally thousands of packages available for\nA function is just a chunck of code, which is wrapped up in a block and given a variable name.\nfoo &lt;- function() { \n  cat(\"bar\")\n}\n\nfoo()\n\nbar\nThe amount of code within a function can be simple like the one above or quite complex. The boundaries of the code are defined by the curly brackets.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "Functions.html#variable-scope",
    "href": "Functions.html#variable-scope",
    "title": "9¬† Functions",
    "section": "Variable Scope",
    "text": "Variable Scope\nWhen we make a function, there is a notion of a scope for variables, which defines where variables are visible from. When we work with functions, we encapsulate code within curly-brackets. This protects their scope. Her is an example. In this function, we:\n\nPrint out the value of a variable x\n\nAssign values to the variables x and z\nPrint out the value of the variables x and z.\n\n\nfoo &lt;- function( ) {\n  x &lt;- 12\n  z &lt;- \"bob\"\n  cat(\"x =\", x, \"& z =\", z ,\"inside function.\\n\")\n}\n\nOK, so now let‚Äôs call this function.\n\nfoo()\n\nx = 12 & z = bob inside function.\n\n\n\nx &lt;- 42\ncat(\"x =\", x, \"before function.\\n\")\n\nx = 42 before function.\n\nfoo()\n\nx = 12 & z = bob inside function.\n\ncat(\"x =\", x, \"after running function.\\n\")\n\nx = 42 after running function.\n\n\nNOTE: The value of x was changed within the function but those changes were not reflected OUTSIDE of that function. The scope of the variable x inside foo() is local to that function and anything that follows its declaration within the curly brackets of the function. However, it is invisible outside the scope of that function. This is a ‚Äògood thing¬©‚Äô because if we had visibility of all the variables in all the functions then we would either a) quickly run out of variable names to keep them unique, or b) clobber all of our existing variables by writing over them and changing their values.\nAlso, notice that the variable z that is assigned bob in the function is also not visible in the global environment. What happens in the function, stays in the function.\n\nls()\n\n[1] \"foo\" \"x\"",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "Functions.html#passing-variables.",
    "href": "Functions.html#passing-variables.",
    "title": "9¬† Functions",
    "section": "Passing Variables.",
    "text": "Passing Variables.\nWhile some functions do not take any input, most require some kind of data to work with or values to start using. These variables can are passed into the function code by including them within the function parentheses.\nAny required variables are added within the function definition parentheses. These translate into the names of the variables used within the chunk.\nHere is an example with one required variable, x.\n\nfoo &lt;- function( x ) {\n  print(x)\n}\n\nAnd it can be called by either naming the variable explicity or not.\n\nfoo( x = 23 )\n\n[1] 23\n\nfoo( 42 )\n\n[1] 42\n\n\nHowever, if you require a variable to be passed and it is not given, it will result in an error.\n\nfoo()\n\nError in `foo()`:\n! argument \"x\" is missing, with no default\n\n\nYou can get around this by making a default value for the variable, which is specified in the function definition as follows:\n\nfoo &lt;- function( x = \"Dr Dyer is my favorite professor\" ) {\n  print(x)\n}\n\nThen if the individual does not fill in\n\nfoo()\n\n[1] \"Dr Dyer is my favorite professor\"",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "Functions.html#retrieving-results-from-functions",
    "href": "Functions.html#retrieving-results-from-functions",
    "title": "9¬† Functions",
    "section": "Retrieving Results from Functions",
    "text": "Retrieving Results from Functions\nSimilarly, many functions we write will return something to the user who is calling it. By default, a function that just does something like print some message or make some plot will return NULL\n\nfoo &lt;- function( name = \"Alice\") {\n  cat(name, \"is in the house.\")\n}\nfoo()\n\nAlice is in the house.\n\n\nBut if I try to assign a variable the results of the function, I get NULL as the value returned.\n\nx &lt;- foo()\n\nAlice is in the house.\n\nclass(x)\n\n[1] \"NULL\"\n\nx\n\nNULL\n\n\nIf you want to return something to the user, you need to be explicit and use the return() function to pass back the variable.\n\nfoo &lt;- function( name = \"Alice\") {\n  response &lt;- paste( name, \"is in the house.\")\n  return( response )\n}\n\n\nwho_is_in_the_house &lt;- foo()\nwho_is_in_the_house\n\n[1] \"Alice is in the house.\"\n\n\nYou can only return one item but it can be a list a data.frame or any other R object.",
    "crumbs": [
      "R Ecosystem",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Functions</span>"
    ]
  },
  {
    "objectID": "FrequentistBayesianInferences.html",
    "href": "FrequentistBayesianInferences.html",
    "title": "15¬† Nature of Statistical Inference",
    "section": "",
    "text": "Frequentist Approaches",
    "crumbs": [
      "Statistical Inferences",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Nature of Statistical Inference</span>"
    ]
  },
  {
    "objectID": "FrequentistBayesianInferences.html#bayesian-inferences",
    "href": "FrequentistBayesianInferences.html#bayesian-inferences",
    "title": "15¬† Nature of Statistical Inference",
    "section": "Bayesian Inferences",
    "text": "Bayesian Inferences",
    "crumbs": [
      "Statistical Inferences",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Nature of Statistical Inference</span>"
    ]
  },
  {
    "objectID": "Correlation.html",
    "href": "Correlation.html",
    "title": "17¬† Correlation",
    "section": "",
    "text": "Parametric",
    "crumbs": [
      "Statistical Inferences",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "Correlation.html#non-parametric",
    "href": "Correlation.html#non-parametric",
    "title": "17¬† Correlation",
    "section": "Non-Parametric",
    "text": "Non-Parametric",
    "crumbs": [
      "Statistical Inferences",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Correlation</span>"
    ]
  },
  {
    "objectID": "DataSets.html",
    "href": "DataSets.html",
    "title": "Appendix A ‚Äî Data Sets",
    "section": "",
    "text": "Example Taxa",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "DataSets.html#example-taxa",
    "href": "DataSets.html#example-taxa",
    "title": "Appendix A ‚Äî Data Sets",
    "section": "",
    "text": "Sonoran Desert Bark Beetle\n\n\n\nFlowering Dogwood",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  },
  {
    "objectID": "DataSets.html#simulated-data",
    "href": "DataSets.html#simulated-data",
    "title": "Appendix A ‚Äî Data Sets",
    "section": "Simulated Data",
    "text": "Simulated Data",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Data Sets</span>"
    ]
  }
]